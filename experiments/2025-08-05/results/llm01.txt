

time=2025-08-05T18:03:03.518-07:00 level=INFO source=routes.go:1238 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LE
NGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LO
ADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/llm/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://loca
lhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-
file://*] OLLAMA_RPC_SERVERS:192.168.0.140:50052,192.168.0.114:50052 OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-08-05T18:03:03.526-07:00 level=INFO source=images.go:476 msg="total blobs: 0"
time=2025-08-05T18:03:03.526-07:00 level=INFO source=images.go:483 msg="total unused blobs removed: 0"
[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.

[GIN-debug] [WARNING] Running in "debug" mode. Switch to "release" mode in production.
 - using env:   export GIN_MODE=release
 - using code:  gin.SetMode(gin.ReleaseMode)

[GIN-debug] HEAD   /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)
[GIN-debug] GET    /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)
[GIN-debug] HEAD   /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func3 (5 handlers)
[GIN-debug] GET    /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func4 (5 handlers)
[GIN-debug] POST   /api/pull                 --> github.com/ollama/ollama/server.(*Server).PullHandler-fm (5 handlers)
[GIN-debug] POST   /api/push                 --> github.com/ollama/ollama/server.(*Server).PushHandler-fm (5 handlers)
[GIN-debug] HEAD   /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers)
[GIN-debug] GET    /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers)
[GIN-debug] POST   /api/show                 --> github.com/ollama/ollama/server.(*Server).ShowHandler-fm (5 handlers)
[GIN-debug] DELETE /api/delete               --> github.com/ollama/ollama/server.(*Server).DeleteHandler-fm (5 handlers)
[GIN-debug] POST   /api/create               --> github.com/ollama/ollama/server.(*Server).CreateHandler-fm (5 handlers)
[GIN-debug] POST   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).CreateBlobHandler-fm (5 handlers)
[GIN-debug] HEAD   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).HeadBlobHandler-fm (5 handlers)
[GIN-debug] POST   /api/copy                 --> github.com/ollama/ollama/server.(*Server).CopyHandler-fm (5 handlers)
[GIN-debug] GET    /api/ps                   --> github.com/ollama/ollama/server.(*Server).PsHandler-fm (5 handlers)
[GIN-debug] POST   /api/generate             --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (5 handlers)
[GIN-debug] POST   /api/chat                 --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (5 handlers)
[GIN-debug] POST   /api/embed                --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (5 handlers)
[GIN-debug] POST   /api/embeddings           --> github.com/ollama/ollama/server.(*Server).EmbeddingsHandler-fm (5 handlers)
[GIN-debug] POST   /v1/chat/completions      --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (6 handlers)
[GIN-debug] POST   /v1/completions           --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (6 handlers)
[GIN-debug] POST   /v1/embeddings            --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (6 handlers)
[GIN-debug] GET    /v1/models                --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (6 handlers)
[GIN-debug] GET    /v1/models/:model         --> github.com/ollama/ollama/server.(*Server).ShowHandler-fm (6 handlers)
time=2025-08-05T18:03:03.677-07:00 level=INFO source=routes.go:1291 msg="Listening on [::]:11434 (version 0.0.0)"
time=2025-08-05T18:03:03.698-07:00 level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-08-05T18:03:04.449-07:00 level=INFO source=types.go:139 msg="inference compute" id=GPU-<ID> library=cuda variant=v12 compute=5.0 driver=13.0 name="NVIDIA GeForce GTX 745" total="3.9 GiB" available
="3.9 GiB"
time=2025-08-05T18:03:04.449-07:00 level=INFO source=types.go:139 msg="inference compute" id=192.168.0.140:50052 library=rpc variant="" compute="" driver=0.0 name="" total="3.8 GiB" available="3.8 GiB"
time=2025-08-05T18:03:04.449-07:00 level=INFO source=types.go:139 msg="inference compute" id=192.168.0.114:50052 library=rpc variant="" compute="" driver=0.0 name="" total="2.9 GiB" available="2.9 GiB"
[GIN] 2025/08/05 - 18:03:34 | 200 |   27.847505ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 18:06:47 | 200 |     198.874µs |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 18:08:26 | 200 |     224.879µs |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 18:08:41 | 200 |     211.698µs |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 18:08:41 | 200 |      60.083µs |   192.168.0.165 | GET      "/"
[GIN] 2025/08/05 - 18:09:09 | 200 |      17.799µs |       127.0.0.1 | HEAD     "/"
time=2025-08-05T18:09:10.600-07:00 level=INFO source=download.go:177 msg="downloading dde5aa3fc5ff in 16 126 MB part(s)"
[GIN] 2025/08/05 - 18:09:47 | 200 |      69.133µs |   192.168.0.165 | GET      "/api/tags"
time=2025-08-05T18:11:35.972-07:00 level=INFO source=download.go:177 msg="downloading 966de95ca8a6 in 1 1.4 KB part(s)"
time=2025-08-05T18:11:37.310-07:00 level=INFO source=download.go:177 msg="downloading fcc5a6bec9da in 1 7.7 KB part(s)"
time=2025-08-05T18:11:38.766-07:00 level=INFO source=download.go:177 msg="downloading a70ff7e570d9 in 1 6.0 KB part(s)"
time=2025-08-05T18:11:40.036-07:00 level=INFO source=download.go:177 msg="downloading 56bb8bd477a5 in 1 96 B part(s)"
time=2025-08-05T18:11:41.323-07:00 level=INFO source=download.go:177 msg="downloading 34bb5ab01051 in 1 561 B part(s)"
[GIN] 2025/08/05 - 18:11:47 | 200 |         2m37s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/05 - 18:11:58 | 200 |      16.845µs |       127.0.0.1 | HEAD     "/"
time=2025-08-05T18:11:59.112-07:00 level=INFO source=download.go:177 msg="downloading e6a7edc1a4d7 in 16 326 MB part(s)"
[GIN] 2025/08/05 - 18:12:56 | 200 |      568.19µs |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 18:16:06 | 200 |     234.265µs |   192.168.0.165 | GET      "/api/tags"
time=2025-08-05T18:18:11.724-07:00 level=INFO source=download.go:177 msg="downloading c5ad996bda6e in 1 556 B part(s)"
time=2025-08-05T18:18:12.981-07:00 level=INFO source=download.go:177 msg="downloading 6e4c38e1172f in 1 1.1 KB part(s)"
time=2025-08-05T18:18:14.253-07:00 level=INFO source=download.go:177 msg="downloading ed8474dc73db in 1 179 B part(s)"
time=2025-08-05T18:18:15.530-07:00 level=INFO source=download.go:177 msg="downloading f64cd5418e4b in 1 487 B part(s)"
[GIN] 2025/08/05 - 18:18:28 | 200 |         6m30s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/05 - 18:18:44 | 200 |    1.177631ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 18:18:44 | 200 |      78.714µs |   192.168.0.165 | GET      "/"
[GIN] 2025/08/05 - 18:18:44 | 200 |     866.559µs |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 18:19:06 | 200 |    1.557708ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 18:22:06 | 200 |    1.751565ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 18:22:07 | 200 |    1.512606ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 18:22:31 | 200 |     705.351µs |   192.168.0.165 | GET      "/api/tags"
time=2025-08-05T18:22:31.735-07:00 level=INFO source=sched.go:814 msg="new model will fit in available VRAM, loading" model=/home/llm/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff library=rpc
 parallel=1 required="5.6 GiB"
time=2025-08-05T18:22:31.778-07:00 level=INFO source=server.go:135 msg="system memory" total="15.6 GiB" free="14.7 GiB" free_swap="976.0 MiB"
time=2025-08-05T18:22:31.778-07:00 level=INFO source=server.go:187 msg=offload library=rpc layers.requested=-1 layers.model=29 layers.offload=29 layers.split=15,14 memory.available="[3.8 GiB 2.9 GiB]" memory.gpu_overhead="0 B" memory.requ
ired.full="5.6 GiB" memory.required.partial="5.6 GiB" memory.required.kv="1.8 GiB" memory.required.allocations="[2.9 GiB 2.6 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" m
emory.graph.full="881.1 MiB" memory.graph.partial="881.1 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/llm/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["   ", "     ", "     ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW)
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 '
'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-05T18:22:32.119-07:00 level=INFO source=server.go:459 msg="starting llama server" cmd="/home/llm/.cache/go-build/50/508af2e93b412cf54a429a67b96dbf2a23d30151a7f377de718962a03c388c18-d/ollama runner --model /home/llm/.ollama/mo
dels/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 16384 --batch-size 512 --n-gpu-layers 29 --rpc 192.168.0.140:50052,192.168.0.114:50052 --threads 4 --no-mmap --parallel 1 --tensor-split 15,14 -
-port 40225"
time=2025-08-05T18:22:32.120-07:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-08-05T18:22:32.120-07:00 level=INFO source=server.go:619 msg="waiting for llama runner to start responding"
time=2025-08-05T18:22:32.120-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-05T18:22:32.128-07:00 level=INFO source=runner.go:816 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce GTX 745, compute capability 5.0, VMM: yes
load_backend: loaded CUDA backend from /home/llm/llm-server/ollama/rpc/ollama-rpc-GH-10844/build/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /home/llm/llm-server/ollama/rpc/ollama-rpc-GH-10844/build/lib/ollama/libggml-cpu-haswell.so
time=2025-08-05T18:22:35.928-07:00 level=INFO source=ggml.go:105 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500 CUDA.0.USE_GRAPHS=1
 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-08-05T18:22:35.929-07:00 level=INFO source=runner.go:876 msg="Server listening on 127.0.0.1:40225"
llama_model_load_from_file_impl: using device RPC[192.168.0.140:50052] (RPC[192.168.0.140:50052]) - 3848 MiB free
llama_model_load_from_file_impl: using device RPC[192.168.0.114:50052] (RPC[192.168.0.114:50052]) - 2968 MiB free
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce GTX 745) - 4005 MiB free
time=2025-08-05T18:22:36.140-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/llm/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["   ", "     ", "     ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW)
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 '
'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors: RPC[192.168.0.140:50052] model buffer size =   852.89 MiB
load_tensors: RPC[192.168.0.114:50052] model buffer size =  1065.46 MiB
load_tensors:          CPU model buffer size =   308.23 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 16384
llama_context: n_ctx_per_seq = 16384
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache_unified: kv_size = 16384, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified: RPC[192.168.0.140:50052] KV buffer size =   960.00 MiB
llama_kv_cache_unified: RPC[192.168.0.114:50052] KV buffer size =   832.00 MiB
llama_kv_cache_unified: KV self size  = 1792.00 MiB, K (f16):  896.00 MiB, V (f16):  896.00 MiB
llama_context: RPC[192.168.0.140:50052] compute buffer size =   824.00 MiB
llama_context: RPC[192.168.0.114:50052] compute buffer size =   824.00 MiB
llama_context:        CPU compute buffer size =    38.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 3
time=2025-08-05T18:23:07.535-07:00 level=INFO source=server.go:658 msg="llama runner started in 35.42 seconds"
[GIN] 2025/08/05 - 18:25:06 | 200 |    1.707331ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 18:28:06 | 200 |    1.689367ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 18:31:06 | 200 |    1.548304ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 18:34:06 | 200 |    1.241512ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 18:37:06 | 200 |    1.159672ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 18:40:06 | 200 |    1.146986ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 18:43:06 | 200 |    1.238463ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 18:46:06 | 200 |    1.168348ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 18:49:06 | 200 |    1.287629ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 18:52:06 | 200 |     748.105µs |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 18:53:02 | 200 |        30m30s |   192.168.0.165 | POST     "/api/chat"
[GIN] 2025/08/05 - 18:53:02 | 200 |     744.967µs |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 18:53:59 | 200 |     1.11726ms |   192.168.0.165 | GET      "/api/tags"
time=2025-08-05T18:54:04.829-07:00 level=ERROR source=gpu_rpc.go:59 msg="failed to fetch RPC server reply size of RPC_CMD_HELLO" err="read tcp 192.168.0.157:40240->192.168.0.140:50052: i/o timeout"
time=2025-08-05T18:54:04.829-07:00 level=WARN source=gpu_rpc.go:177 msg="unable to connect to endpoint" endpoint=192.168.0.140:50052
time=2025-08-05T18:54:09.834-07:00 level=ERROR source=gpu_rpc.go:59 msg="failed to fetch RPC server reply size of RPC_CMD_HELLO" err="read tcp 192.168.0.157:46864->192.168.0.114:50052: i/o timeout"
time=2025-08-05T18:54:09.834-07:00 level=WARN source=gpu_rpc.go:177 msg="unable to connect to endpoint" endpoint=192.168.0.114:50052
time=2025-08-05T18:54:09.887-07:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-<ID> library=cuda total="3.9 GiB" available="3.9 GiB"
[GIN] 2025/08/05 - 18:55:06 | 200 |     988.765µs |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 18:58:06 | 200 |    1.031435ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:01:06 | 200 |    1.036172ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:04:06 | 200 |    1.058044ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:06:03 | 200 |         13m0s |   192.168.0.165 | POST     "/api/chat"
time=2025-08-05T19:06:08.058-07:00 level=WARN source=sched.go:697 msg="gpu VRAM usage didn't recover within timeout" seconds=5.044988724 runner.size="5.6 GiB" runner.vram="5.6 GiB" runner.parallel=1 runner.pid=1543 runner.model=/home/llm/
.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff
time=2025-08-05T19:06:08.218-07:00 level=INFO source=server.go:135 msg="system memory" total="15.6 GiB" free="14.6 GiB" free_swap="976.0 MiB"
time=2025-08-05T19:06:08.219-07:00 level=INFO source=server.go:187 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=28 layers.split=9,12,7 memory.available="[3.9 GiB 3.8 GiB 2.9 GiB]" memory.gpu_overhead="0 B" m
emory.required.full="12.3 GiB" memory.required.partial="10.4 GiB" memory.required.kv="2.2 GiB" memory.required.allocations="[3.7 GiB 3.7 GiB 2.9 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrep
eating="486.9 MiB" memory.graph.full="1.5 GiB" memory.graph.partial="1.5 GiB"
llama_model_loader: loaded meta data with 32 key-value pairs and 399 tensors from /home/llm/.ollama/models/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 0528 Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-0528-Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = mit
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 131072
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                    qwen3.rope.scaling.type str              = yarn
llama_model_loader: - kv  17:                  qwen3.rope.scaling.factor f32              = 4.000000
llama_model_loader: - kv  18: qwen3.rope.scaling.original_context_length u32              = 32768
llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,151387]  = ["   ", "     ", "i n", "  t",...
llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 151645
llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  30:               general.quantization_version u32              = 2
llama_model_loader: - kv  31:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW)
time=2025-08-05T19:06:08.307-07:00 level=WARN source=sched.go:697 msg="gpu VRAM usage didn't recover within timeout" seconds=5.29473279 runner.size="5.6 GiB" runner.vram="5.6 GiB" runner.parallel=1 runner.pid=1543 runner.model=/home/llm/.
ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 28
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = DeepSeek R1 0528 Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<\ÿbeginofsentence\ÿ>'
print_info: EOS token        = 151645 '<\ÿendofsentence\ÿ>'
print_info: EOT token        = 151645 '<\ÿendofsentence\ÿ>'
print_info: PAD token        = 151645 '<\ÿendofsentence\ÿ>'
print_info: LF token         = 198 '
'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151645 '<\ÿendofsentence\ÿ>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-05T19:06:08.459-07:00 level=INFO source=server.go:459 msg="starting llama server" cmd="/home/llm/.cache/go-build/50/508af2e93b412cf54a429a67b96dbf2a23d30151a7f377de718962a03c388c18-d/ollama runner --model /home/llm/.ollama/mo
dels/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d --ctx-size 16384 --batch-size 512 --n-gpu-layers 28 --rpc 192.168.0.140:50052,192.168.0.114:50052 --threads 4 --no-mmap --parallel 1 --tensor-split 9,12,7
--port 45829"
time=2025-08-05T19:06:08.459-07:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-08-05T19:06:08.459-07:00 level=INFO source=server.go:619 msg="waiting for llama runner to start responding"
time=2025-08-05T19:06:08.459-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-05T19:06:08.467-07:00 level=INFO source=runner.go:816 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce GTX 745, compute capability 5.0, VMM: yes
load_backend: loaded CUDA backend from /home/llm/llm-server/ollama/rpc/ollama-rpc-GH-10844/build/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /home/llm/llm-server/ollama/rpc/ollama-rpc-GH-10844/build/lib/ollama/libggml-cpu-haswell.so
time=2025-08-05T19:06:08.505-07:00 level=INFO source=ggml.go:105 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500 CUDA.0.USE_GRAPHS=1
 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-08-05T19:06:08.505-07:00 level=INFO source=runner.go:876 msg="Server listening on 127.0.0.1:45829"
llama_model_load_from_file_impl: using device RPC[192.168.0.140:50052] (RPC[192.168.0.140:50052]) - 3848 MiB free
llama_model_load_from_file_impl: using device RPC[192.168.0.114:50052] (RPC[192.168.0.114:50052]) - 2968 MiB free
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce GTX 745) - 4005 MiB free
time=2025-08-05T19:06:08.557-07:00 level=WARN source=sched.go:697 msg="gpu VRAM usage didn't recover within timeout" seconds=5.54456609 runner.size="5.6 GiB" runner.vram="5.6 GiB" runner.parallel=1 runner.pid=1543 runner.model=/home/llm/.
ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff
llama_model_loader: loaded meta data with 32 key-value pairs and 399 tensors from /home/llm/.ollama/models/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 0528 Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-0528-Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = mit
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 131072
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                    qwen3.rope.scaling.type str              = yarn
llama_model_loader: - kv  17:                  qwen3.rope.scaling.factor f32              = 4.000000
llama_model_loader: - kv  18: qwen3.rope.scaling.original_context_length u32              = 32768
llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,151387]  = ["   ", "     ", "i n", "  t",...
llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 151645
llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  30:               general.quantization_version u32              = 2
llama_model_loader: - kv  31:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW)
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 28
time=2025-08-05T19:06:08.710-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = yarn
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 0.25
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = DeepSeek R1 0528 Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<\ÿbeginofsentence\ÿ>'
print_info: EOS token        = 151645 '<\ÿendofsentence\ÿ>'
print_info: EOT token        = 151645 '<\ÿendofsentence\ÿ>'
print_info: PAD token        = 151645 '<\ÿendofsentence\ÿ>'
print_info: LF token         = 198 '
'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151645 '<\ÿendofsentence\ÿ>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloaded 28/37 layers to GPU
load_tensors: RPC[192.168.0.140:50052] model buffer size =  1020.67 MiB
load_tensors: RPC[192.168.0.114:50052] model buffer size =  1360.89 MiB
load_tensors:    CUDA_Host model buffer size =  1423.00 MiB
load_tensors:        CUDA0 model buffer size =   839.23 MiB
load_tensors:          CPU model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 16384
llama_context: n_ctx_per_seq = 16384
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 0.25
llama_context: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 16384, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified: RPC[192.168.0.140:50052] KV buffer size =   576.00 MiB
llama_kv_cache_unified: RPC[192.168.0.114:50052] KV buffer size =   768.00 MiB
llama_kv_cache_unified:      CUDA0 KV buffer size =   448.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =   512.00 MiB
llama_kv_cache_unified: KV self size  = 2304.00 MiB, K (f16): 1152.00 MiB, V (f16): 1152.00 MiB
llama_context: RPC[192.168.0.140:50052] compute buffer size =  1088.00 MiB
llama_context: RPC[192.168.0.114:50052] compute buffer size =  1088.00 MiB
llama_context:      CUDA0 compute buffer size =  1176.00 MiB
llama_context:        CPU compute buffer size =    40.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 111 (with bs=512), 21 (with bs=1)
time=2025-08-05T19:06:43.616-07:00 level=INFO source=server.go:658 msg="llama runner started in 35.16 seconds"
[GIN] 2025/08/05 - 19:07:07 | 200 |    1.058873ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:10:07 | 200 |    1.039363ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:13:07 | 200 |    1.193022ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:16:07 | 200 |      1.2622ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:19:07 | 200 |    1.239639ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:22:07 | 200 |    1.211871ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:25:07 | 200 |     1.34401ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:28:07 | 200 |    1.085446ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:31:07 | 200 |    1.062902ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:34:07 | 200 |    1.218554ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:37:07 | 200 |    1.193993ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:40:07 | 200 |    1.464056ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:42:28 | 200 |        48m29s |   192.168.0.165 | POST     "/api/chat"
[GIN] 2025/08/05 - 19:42:28 | 200 |     327.276µs |   192.168.0.165 | GET      "/api/tags"
time=2025-08-05T19:42:34.060-07:00 level=ERROR source=gpu_rpc.go:59 msg="failed to fetch RPC server reply size of RPC_CMD_HELLO" err="read tcp 192.168.0.157:42620->192.168.0.140:50052: i/o timeout"
time=2025-08-05T19:42:34.061-07:00 level=WARN source=gpu_rpc.go:177 msg="unable to connect to endpoint" endpoint=192.168.0.140:50052
time=2025-08-05T19:42:39.062-07:00 level=ERROR source=gpu_rpc.go:59 msg="failed to fetch RPC server reply size of RPC_CMD_HELLO" err="read tcp 192.168.0.157:51650->192.168.0.114:50052: i/o timeout"
time=2025-08-05T19:42:39.063-07:00 level=WARN source=gpu_rpc.go:177 msg="unable to connect to endpoint" endpoint=192.168.0.114:50052
time=2025-08-05T19:42:39.126-07:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-<ID> library=cuda total="3.9 GiB" available="197.7 MiB"
time=2025-08-05T19:42:44.164-07:00 level=WARN source=sched.go:697 msg="gpu VRAM usage didn't recover within timeout" seconds=5.037687018 runner.size="12.3 GiB" runner.vram="10.4 GiB" runner.parallel=1 runner.pid=1647 runner.model=/home/ll
m/.ollama/models/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d
time=2025-08-05T19:42:44.290-07:00 level=INFO source=sched.go:814 msg="new model will fit in available VRAM, loading" model=/home/llm/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff library=rpc
 parallel=1 required="5.6 GiB"
time=2025-08-05T19:42:44.324-07:00 level=INFO source=server.go:135 msg="system memory" total="15.6 GiB" free="14.6 GiB" free_swap="976.0 MiB"
time=2025-08-05T19:42:44.324-07:00 level=INFO source=server.go:187 msg=offload library=rpc layers.requested=-1 layers.model=29 layers.offload=29 layers.split=15,14 memory.available="[3.8 GiB 2.9 GiB]" memory.gpu_overhead="0 B" memory.requ
ired.full="5.6 GiB" memory.required.partial="5.6 GiB" memory.required.kv="1.8 GiB" memory.required.allocations="[2.9 GiB 2.6 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" m
emory.graph.full="881.1 MiB" memory.graph.partial="881.1 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/llm/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["   ", "     ", "     ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW)
time=2025-08-05T19:42:44.414-07:00 level=WARN source=sched.go:697 msg="gpu VRAM usage didn't recover within timeout" seconds=5.287788426 runner.size="12.3 GiB" runner.vram="10.4 GiB" runner.parallel=1 runner.pid=1647 runner.model=/home/ll
m/.ollama/models/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 '
'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-05T19:42:44.609-07:00 level=INFO source=server.go:459 msg="starting llama server" cmd="/home/llm/.cache/go-build/50/508af2e93b412cf54a429a67b96dbf2a23d30151a7f377de718962a03c388c18-d/ollama runner --model /home/llm/.ollama/mo
dels/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 16384 --batch-size 512 --n-gpu-layers 29 --rpc 192.168.0.140:50052,192.168.0.114:50052 --threads 4 --no-mmap --parallel 1 --tensor-split 15,14 -
-port 33529"
time=2025-08-05T19:42:44.609-07:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-08-05T19:42:44.609-07:00 level=INFO source=server.go:619 msg="waiting for llama runner to start responding"
time=2025-08-05T19:42:44.609-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-05T19:42:44.617-07:00 level=INFO source=runner.go:816 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce GTX 745, compute capability 5.0, VMM: yes
load_backend: loaded CUDA backend from /home/llm/llm-server/ollama/rpc/ollama-rpc-GH-10844/build/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /home/llm/llm-server/ollama/rpc/ollama-rpc-GH-10844/build/lib/ollama/libggml-cpu-haswell.so
time=2025-08-05T19:42:44.656-07:00 level=INFO source=ggml.go:105 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500 CUDA.0.USE_GRAPHS=1
 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-08-05T19:42:44.656-07:00 level=INFO source=runner.go:876 msg="Server listening on 127.0.0.1:33529"
llama_model_load_from_file_impl: using device RPC[192.168.0.140:50052] (RPC[192.168.0.140:50052]) - 3848 MiB free
llama_model_load_from_file_impl: using device RPC[192.168.0.114:50052] (RPC[192.168.0.114:50052]) - 2968 MiB free
time=2025-08-05T19:42:44.664-07:00 level=WARN source=sched.go:697 msg="gpu VRAM usage didn't recover within timeout" seconds=5.537974233 runner.size="12.3 GiB" runner.vram="10.4 GiB" runner.parallel=1 runner.pid=1647 runner.model=/home/ll
m/.ollama/models/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce GTX 745) - 4005 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/llm/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["   ", "     ", "     ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW)
time=2025-08-05T19:42:44.860-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 '
'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors: RPC[192.168.0.140:50052] model buffer size =   852.89 MiB
load_tensors: RPC[192.168.0.114:50052] model buffer size =  1065.46 MiB
load_tensors:          CPU model buffer size =   308.23 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 16384
llama_context: n_ctx_per_seq = 16384
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache_unified: kv_size = 16384, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified: RPC[192.168.0.140:50052] KV buffer size =   960.00 MiB
llama_kv_cache_unified: RPC[192.168.0.114:50052] KV buffer size =   832.00 MiB
llama_kv_cache_unified: KV self size  = 1792.00 MiB, K (f16):  896.00 MiB, V (f16):  896.00 MiB
llama_context: RPC[192.168.0.140:50052] compute buffer size =   824.00 MiB
llama_context: RPC[192.168.0.114:50052] compute buffer size =   824.00 MiB
llama_context:        CPU compute buffer size =    38.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 3
time=2025-08-05T19:43:06.969-07:00 level=INFO source=server.go:658 msg="llama runner started in 22.36 seconds"
[GIN] 2025/08/05 - 19:43:07 | 200 |    1.459971ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:46:07 | 200 |     1.48696ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:49:07 | 200 |    1.301561ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:52:07 | 200 |    1.098278ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:52:52 | 200 |        10m23s |   192.168.0.165 | POST     "/api/chat"
[GIN] 2025/08/05 - 19:53:51 | 200 |    1.573275ms |   192.168.0.165 | GET      "/api/tags"
time=2025-08-05T19:53:56.133-07:00 level=ERROR source=gpu_rpc.go:59 msg="failed to fetch RPC server reply size of RPC_CMD_HELLO" err="read tcp 192.168.0.157:43374->192.168.0.140:50052: i/o timeout"
time=2025-08-05T19:53:56.134-07:00 level=WARN source=gpu_rpc.go:177 msg="unable to connect to endpoint" endpoint=192.168.0.140:50052
time=2025-08-05T19:54:01.135-07:00 level=ERROR source=gpu_rpc.go:59 msg="failed to fetch RPC server reply size of RPC_CMD_HELLO" err="read tcp 192.168.0.157:36922->192.168.0.114:50052: i/o timeout"
time=2025-08-05T19:54:01.135-07:00 level=WARN source=gpu_rpc.go:177 msg="unable to connect to endpoint" endpoint=192.168.0.114:50052
time=2025-08-05T19:54:01.188-07:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-<ID> library=cuda total="3.9 GiB" available="3.9 GiB"
time=2025-08-05T19:54:06.227-07:00 level=WARN source=sched.go:697 msg="gpu VRAM usage didn't recover within timeout" seconds=5.037862589 runner.size="5.6 GiB" runner.vram="5.6 GiB" runner.parallel=1 runner.pid=10197 runner.model=/home/llm
/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff
time=2025-08-05T19:54:06.348-07:00 level=INFO source=server.go:135 msg="system memory" total="15.6 GiB" free="14.5 GiB" free_swap="976.0 MiB"
time=2025-08-05T19:54:06.349-07:00 level=INFO source=server.go:187 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=28 layers.split=9,12,7 memory.available="[3.9 GiB 3.8 GiB 2.9 GiB]" memory.gpu_overhead="0 B" m
emory.required.full="12.3 GiB" memory.required.partial="10.4 GiB" memory.required.kv="2.2 GiB" memory.required.allocations="[3.7 GiB 3.7 GiB 2.9 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrep
eating="486.9 MiB" memory.graph.full="1.5 GiB" memory.graph.partial="1.5 GiB"
llama_model_loader: loaded meta data with 32 key-value pairs and 399 tensors from /home/llm/.ollama/models/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 0528 Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-0528-Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = mit
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 131072
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                    qwen3.rope.scaling.type str              = yarn
llama_model_loader: - kv  17:                  qwen3.rope.scaling.factor f32              = 4.000000
llama_model_loader: - kv  18: qwen3.rope.scaling.original_context_length u32              = 32768
llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,151387]  = ["   ", "     ", "i n", "  t",...
llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 151645
llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  30:               general.quantization_version u32              = 2
llama_model_loader: - kv  31:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW)
time=2025-08-05T19:54:06.477-07:00 level=WARN source=sched.go:697 msg="gpu VRAM usage didn't recover within timeout" seconds=5.288219411 runner.size="5.6 GiB" runner.vram="5.6 GiB" runner.parallel=1 runner.pid=10197 runner.model=/home/llm
/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 28
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = DeepSeek R1 0528 Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<\ÿbeginofsentence\ÿ>'
print_info: EOS token        = 151645 '<\ÿendofsentence\ÿ>'
print_info: EOT token        = 151645 '<\ÿendofsentence\ÿ>'
print_info: PAD token        = 151645 '<\ÿendofsentence\ÿ>'
print_info: LF token         = 198 '
'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151645 '<\ÿendofsentence\ÿ>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-05T19:54:06.581-07:00 level=INFO source=server.go:459 msg="starting llama server" cmd="/home/llm/.cache/go-build/50/508af2e93b412cf54a429a67b96dbf2a23d30151a7f377de718962a03c388c18-d/ollama runner --model /home/llm/.ollama/mo
dels/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d --ctx-size 16384 --batch-size 512 --n-gpu-layers 28 --rpc 192.168.0.140:50052,192.168.0.114:50052 --threads 4 --no-mmap --parallel 1 --tensor-split 9,12,7
--port 40045"
time=2025-08-05T19:54:06.581-07:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-08-05T19:54:06.581-07:00 level=INFO source=server.go:619 msg="waiting for llama runner to start responding"
time=2025-08-05T19:54:06.582-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-05T19:54:06.590-07:00 level=INFO source=runner.go:816 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce GTX 745, compute capability 5.0, VMM: yes
load_backend: loaded CUDA backend from /home/llm/llm-server/ollama/rpc/ollama-rpc-GH-10844/build/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /home/llm/llm-server/ollama/rpc/ollama-rpc-GH-10844/build/lib/ollama/libggml-cpu-haswell.so
time=2025-08-05T19:54:06.630-07:00 level=INFO source=ggml.go:105 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500 CUDA.0.USE_GRAPHS=1
 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-08-05T19:54:06.630-07:00 level=INFO source=runner.go:876 msg="Server listening on 127.0.0.1:40045"
llama_model_load_from_file_impl: using device RPC[192.168.0.140:50052] (RPC[192.168.0.140:50052]) - 3848 MiB free
llama_model_load_from_file_impl: using device RPC[192.168.0.114:50052] (RPC[192.168.0.114:50052]) - 2968 MiB free
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce GTX 745) - 4005 MiB free
llama_model_loader: loaded meta data with 32 key-value pairs and 399 tensors from /home/llm/.ollama/models/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 0528 Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-0528-Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = mit
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 131072
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                    qwen3.rope.scaling.type str              = yarn
llama_model_loader: - kv  17:                  qwen3.rope.scaling.factor f32              = 4.000000
llama_model_loader: - kv  18: qwen3.rope.scaling.original_context_length u32              = 32768
llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
time=2025-08-05T19:54:06.727-07:00 level=WARN source=sched.go:697 msg="gpu VRAM usage didn't recover within timeout" seconds=5.538002935 runner.size="5.6 GiB" runner.vram="5.6 GiB" runner.parallel=1 runner.pid=10197 runner.model=/home/llm
/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff
llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,151387]  = ["   ", "     ", "i n", "  t",...
llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 151645
llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  30:               general.quantization_version u32              = 2
llama_model_loader: - kv  31:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW)
time=2025-08-05T19:54:06.833-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 28
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = yarn
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 0.25
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = DeepSeek R1 0528 Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<\ÿbeginofsentence\ÿ>'
print_info: EOS token        = 151645 '<\ÿendofsentence\ÿ>'
print_info: EOT token        = 151645 '<\ÿendofsentence\ÿ>'
print_info: PAD token        = 151645 '<\ÿendofsentence\ÿ>'
print_info: LF token         = 198 '
'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151645 '<\ÿendofsentence\ÿ>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloaded 28/37 layers to GPU
load_tensors: RPC[192.168.0.140:50052] model buffer size =  1020.67 MiB
load_tensors: RPC[192.168.0.114:50052] model buffer size =  1360.89 MiB
load_tensors:    CUDA_Host model buffer size =  1423.00 MiB
load_tensors:        CUDA0 model buffer size =   839.23 MiB
load_tensors:          CPU model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 16384
llama_context: n_ctx_per_seq = 16384
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 0.25
llama_context: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 16384, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified: RPC[192.168.0.140:50052] KV buffer size =   576.00 MiB
llama_kv_cache_unified: RPC[192.168.0.114:50052] KV buffer size =   768.00 MiB
llama_kv_cache_unified:      CUDA0 KV buffer size =   448.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =   512.00 MiB
llama_kv_cache_unified: KV self size  = 2304.00 MiB, K (f16): 1152.00 MiB, V (f16): 1152.00 MiB
llama_context: RPC[192.168.0.140:50052] compute buffer size =  1088.00 MiB
llama_context: RPC[192.168.0.114:50052] compute buffer size =  1088.00 MiB
llama_context:      CUDA0 compute buffer size =  1176.00 MiB
llama_context:        CPU compute buffer size =    40.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 111 (with bs=512), 21 (with bs=1)
time=2025-08-05T19:54:32.957-07:00 level=INFO source=server.go:658 msg="llama runner started in 26.38 seconds"
[GIN] 2025/08/05 - 19:55:07 | 200 |    1.476749ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:56:35 | 200 |      986.35µs |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:58:07 | 200 |    1.318113ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:59:31 | 200 |     1.56511ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:59:31 | 200 |    1.978068ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:59:31 | 200 |    1.725841ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:59:31 | 200 |    2.046017ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:59:31 | 200 |    3.056973ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 19:59:31 | 200 |    2.295961ms |   192.168.0.165 | GET      "/api/tags"
time=2025-08-05T19:59:36.147-07:00 level=ERROR source=gpu_rpc.go:59 msg="failed to fetch RPC server reply size of RPC_CMD_HELLO" err="read tcp 192.168.0.157:53484->192.168.0.140:50052: i/o timeout"
time=2025-08-05T19:59:36.147-07:00 level=WARN source=gpu_rpc.go:177 msg="unable to connect to endpoint" endpoint=192.168.0.140:50052
time=2025-08-05T19:59:41.152-07:00 level=ERROR source=gpu_rpc.go:59 msg="failed to fetch RPC server reply size of RPC_CMD_HELLO" err="read tcp 192.168.0.157:45488->192.168.0.114:50052: i/o timeout"
time=2025-08-05T19:59:41.152-07:00 level=WARN source=gpu_rpc.go:177 msg="unable to connect to endpoint" endpoint=192.168.0.114:50052
time=2025-08-05T19:59:41.210-07:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-<ID> library=cuda total="3.9 GiB" available="197.7 MiB"
[GIN] 2025/08/05 - 20:00:30 | 200 |    1.314311ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 20:01:07 | 200 |     1.15709ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 20:04:07 | 200 |    1.506273ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 20:07:07 | 200 |    1.267352ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 20:09:01 | 200 |    1.029599ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 20:10:07 | 200 |    1.269762ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 20:13:07 | 200 |     590.388µs |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 20:16:07 | 200 |     1.24915ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 20:19:07 | 200 |    1.300859ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 20:22:07 | 200 |    1.246229ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 20:25:07 | 200 |     1.02547ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 20:28:07 | 200 |     1.13567ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/05 - 20:31:07 | 200 |    1.133699ms |   192.168.0.165 | GET      "/api/tags"


