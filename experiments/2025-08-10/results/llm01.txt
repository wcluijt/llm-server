

time=2025-08-10T21:33:44.250-07:00 level=INFO source=routes.go:1304 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LE
NGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LO
ADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/llm/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://loca
lhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-
file://*] OLLAMA_RPC_SERVERS:192.168.0.140:50052,192.168.0.114:50052 OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-08-10T21:33:44.254-07:00 level=INFO source=images.go:477 msg="total blobs: 0"
time=2025-08-10T21:33:44.254-07:00 level=INFO source=images.go:484 msg="total unused blobs removed: 0"
[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.

[GIN-debug] [WARNING] Running in "debug" mode. Switch to "release" mode in production.
 - using env:   export GIN_MODE=release
 - using code:  gin.SetMode(gin.ReleaseMode)

[GIN-debug] HEAD   /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)
[GIN-debug] GET    /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)
[GIN-debug] HEAD   /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func3 (5 handlers)
[GIN-debug] GET    /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func4 (5 handlers)
[GIN-debug] POST   /api/pull                 --> github.com/ollama/ollama/server.(*Server).PullHandler-fm (5 handlers)
[GIN-debug] POST   /api/push                 --> github.com/ollama/ollama/server.(*Server).PushHandler-fm (5 handlers)
[GIN-debug] HEAD   /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers)
[GIN-debug] GET    /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers)
[GIN-debug] POST   /api/show                 --> github.com/ollama/ollama/server.(*Server).ShowHandler-fm (5 handlers)
[GIN-debug] DELETE /api/delete               --> github.com/ollama/ollama/server.(*Server).DeleteHandler-fm (5 handlers)
[GIN-debug] POST   /api/create               --> github.com/ollama/ollama/server.(*Server).CreateHandler-fm (5 handlers)
[GIN-debug] POST   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).CreateBlobHandler-fm (5 handlers)
[GIN-debug] HEAD   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).HeadBlobHandler-fm (5 handlers)
[GIN-debug] POST   /api/copy                 --> github.com/ollama/ollama/server.(*Server).CopyHandler-fm (5 handlers)
[GIN-debug] GET    /api/ps                   --> github.com/ollama/ollama/server.(*Server).PsHandler-fm (5 handlers)
[GIN-debug] POST   /api/generate             --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (5 handlers)
[GIN-debug] POST   /api/chat                 --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (5 handlers)
[GIN-debug] POST   /api/embed                --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (5 handlers)
[GIN-debug] POST   /api/embeddings           --> github.com/ollama/ollama/server.(*Server).EmbeddingsHandler-fm (5 handlers)
[GIN-debug] POST   /v1/chat/completions      --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (6 handlers)
[GIN-debug] POST   /v1/completions           --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (6 handlers)
[GIN-debug] POST   /v1/embeddings            --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (6 handlers)
[GIN-debug] GET    /v1/models                --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (6 handlers)
[GIN-debug] GET    /v1/models/:model         --> github.com/ollama/ollama/server.(*Server).ShowHandler-fm (6 handlers)
time=2025-08-10T21:33:44.283-07:00 level=INFO source=routes.go:1357 msg="Listening on [::]:11434 (version 0.0.0)"
time=2025-08-10T21:33:44.321-07:00 level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-08-10T21:33:45.169-07:00 level=INFO source=types.go:139 msg="inference compute" id=GPU-<ID> library=cuda variant=v12 compute=5.0 driver=13.0 name="NVIDIA GeForce GTX 745" total="3.9 GiB" available
="3.9 GiB"
time=2025-08-10T21:33:45.169-07:00 level=INFO source=types.go:139 msg="inference compute" id=192.168.0.140:50052 library=rpc variant="" compute="" driver=0.0 name="" total="3.8 GiB" available="3.8 GiB"
time=2025-08-10T21:33:45.169-07:00 level=INFO source=types.go:139 msg="inference compute" id=192.168.0.114:50052 library=rpc variant="" compute="" driver=0.0 name="" total="2.9 GiB" available="2.9 GiB"
time=2025-08-10T21:33:45.169-07:00 level=INFO source=routes.go:1401 msg="entering low vram mode" "total vram"="10.6 GiB" threshold="20.0 GiB"
[GIN] 2025/08/10 - 21:33:51 | 200 |   62.669451ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:34:50 | 200 |       21.05µs |       127.0.0.1 | HEAD     "/"
time=2025-08-10T21:34:51.309-07:00 level=INFO source=download.go:177 msg="downloading dde5aa3fc5ff in 16 126 MB part(s)"
[GIN] 2025/08/10 - 21:36:55 | 200 |      73.986µs |   192.168.0.165 | GET      "/api/tags"
time=2025-08-10T21:37:21.840-07:00 level=INFO source=download.go:177 msg="downloading 966de95ca8a6 in 1 1.4 KB part(s)"
time=2025-08-10T21:37:23.137-07:00 level=INFO source=download.go:177 msg="downloading fcc5a6bec9da in 1 7.7 KB part(s)"
time=2025-08-10T21:37:24.391-07:00 level=INFO source=download.go:177 msg="downloading a70ff7e570d9 in 1 6.0 KB part(s)"
time=2025-08-10T21:37:25.650-07:00 level=INFO source=download.go:177 msg="downloading 56bb8bd477a5 in 1 96 B part(s)"
time=2025-08-10T21:37:26.929-07:00 level=INFO source=download.go:177 msg="downloading 34bb5ab01051 in 1 561 B part(s)"
[GIN] 2025/08/10 - 21:37:32 | 200 |         2m42s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/10 - 21:37:45 | 200 |      18.064µs |       127.0.0.1 | HEAD     "/"
time=2025-08-10T21:37:46.471-07:00 level=INFO source=download.go:177 msg="downloading e6a7edc1a4d7 in 16 326 MB part(s)"
[GIN] 2025/08/10 - 21:40:06 | 200 |     307.485µs |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:43:18 | 200 |     278.999µs |   192.168.0.165 | GET      "/api/tags"
time=2025-08-10T21:44:18.001-07:00 level=INFO source=download.go:177 msg="downloading c5ad996bda6e in 1 556 B part(s)"
time=2025-08-10T21:44:19.275-07:00 level=INFO source=download.go:177 msg="downloading 6e4c38e1172f in 1 1.1 KB part(s)"
time=2025-08-10T21:44:20.543-07:00 level=INFO source=download.go:177 msg="downloading ed8474dc73db in 1 179 B part(s)"
time=2025-08-10T21:44:21.785-07:00 level=INFO source=download.go:177 msg="downloading f64cd5418e4b in 1 487 B part(s)"
[GIN] 2025/08/10 - 21:44:35 | 200 |         6m49s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/10 - 21:44:44 | 200 |      23.994µs |       127.0.0.1 | HEAD     "/"
time=2025-08-10T21:44:44.994-07:00 level=INFO source=download.go:177 msg="downloading b112e727c6f1 in 16 861 MB part(s)"
[GIN] 2025/08/10 - 21:46:28 | 200 |     367.452µs |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:49:44 | 200 |      380.44µs |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:52:49 | 200 |     402.123µs |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:56:05 | 200 |     417.788µs |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:59:12 | 200 |     375.613µs |   192.168.0.165 | GET      "/api/tags"
time=2025-08-10T22:01:28.557-07:00 level=INFO source=download.go:177 msg="downloading fa6710a93d78 in 1 7.2 KB part(s)"
time=2025-08-10T22:01:29.822-07:00 level=INFO source=download.go:177 msg="downloading f60356777647 in 1 11 KB part(s)"
time=2025-08-10T22:01:31.076-07:00 level=INFO source=download.go:177 msg="downloading 12e88b2a8727 in 1 28 B part(s)"
time=2025-08-10T22:01:32.343-07:00 level=INFO source=download.go:177 msg="downloading 6b0e0f9ced3b in 1 489 B part(s)"
[GIN] 2025/08/10 - 22:02:05 | 200 |        17m21s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/10 - 22:02:25 | 200 |    1.520481ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:03:44 | 200 |    1.290589ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:04:00 | 200 |    1.076929ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:04:13 | 200 |      49.414µs |   192.168.0.165 | GET      "/"
[GIN] 2025/08/10 - 22:04:13 | 200 |    1.365852ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:04:41 | 200 |    1.356271ms |   192.168.0.165 | GET      "/api/tags"
time=2025-08-10T22:04:42.472-07:00 level=INFO source=sched.go:814 msg="new model will fit in available VRAM, loading" model=/home/llm/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff library=rpc
 parallel=1 required="5.6 GiB"
time=2025-08-10T22:04:42.544-07:00 level=INFO source=server.go:135 msg="system memory" total="15.6 GiB" free="14.7 GiB" free_swap="976.0 MiB"
time=2025-08-10T22:04:42.545-07:00 level=INFO source=server.go:187 msg=offload library=rpc layers.requested=-1 layers.model=29 layers.offload=29 layers.split=15,14 memory.available="[3.8 GiB 2.9 GiB]" memory.gpu_overhead="0 B" memory.requ
ired.full="5.6 GiB" memory.required.partial="5.6 GiB" memory.required.kv="1.8 GiB" memory.required.allocations="[2.9 GiB 2.6 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" m
emory.graph.full="881.1 MiB" memory.graph.partial="881.1 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/llm/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["   ", "     ", "     ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW)
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 '
'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-10T22:04:42.960-07:00 level=INFO source=server.go:459 msg="starting llama server" cmd="/home/llm/.cache/go-build/02/02d222919dcb78c0bc6a0c058acebcc67c58bf245a3ca25fb7d48db2b2b875cb-d/ollama runner --model /home/llm/.ollama/mo
dels/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 16384 --batch-size 512 --n-gpu-layers 29 --rpc 192.168.0.140:50052,192.168.0.114:50052 --threads 4 --no-mmap --parallel 1 --tensor-split 15,14 -
-port 38825"
time=2025-08-10T22:04:42.960-07:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-08-10T22:04:42.960-07:00 level=INFO source=server.go:619 msg="waiting for llama runner to start responding"
time=2025-08-10T22:04:42.960-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-10T22:04:43.062-07:00 level=INFO source=runner.go:816 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce GTX 745, compute capability 5.0, VMM: yes
load_backend: loaded CUDA backend from /home/llm/llm-server/ollama/rpc/ollama-rpc-GH-10844/build/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /home/llm/llm-server/ollama/rpc/ollama-rpc-GH-10844/build/lib/ollama/libggml-cpu-haswell.so
time=2025-08-10T22:04:46.200-07:00 level=INFO source=ggml.go:105 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500 CUDA.0.USE_GRAPHS=1
 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
llama_model_load_from_file_impl: using device RPC[192.168.0.140:50052] (RPC[192.168.0.140:50052]) - 3848 MiB free
llama_model_load_from_file_impl: using device RPC[192.168.0.114:50052] (RPC[192.168.0.114:50052]) - 2968 MiB free
time=2025-08-10T22:04:46.225-07:00 level=INFO source=runner.go:876 msg="Server listening on 127.0.0.1:38825"
time=2025-08-10T22:04:46.226-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce GTX 745) - 4005 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/llm/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["   ", "     ", "     ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW)
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 '
'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors: RPC[192.168.0.140:50052] model buffer size =   852.89 MiB
load_tensors: RPC[192.168.0.114:50052] model buffer size =  1065.46 MiB
load_tensors:          CPU model buffer size =   308.23 MiB
[GIN] 2025/08/10 - 22:05:25 | 200 |     418.928µs |   192.168.0.165 | GET      "/api/tags"
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 16384
llama_context: n_ctx_per_seq = 16384
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache_unified: kv_size = 16384, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified: RPC[192.168.0.140:50052] KV buffer size =   960.00 MiB
llama_kv_cache_unified: RPC[192.168.0.114:50052] KV buffer size =   832.00 MiB
llama_kv_cache_unified: KV self size  = 1792.00 MiB, K (f16):  896.00 MiB, V (f16):  896.00 MiB
llama_context: RPC[192.168.0.140:50052] compute buffer size =   824.00 MiB
llama_context: RPC[192.168.0.114:50052] compute buffer size =   824.00 MiB
llama_context:        CPU compute buffer size =    38.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 3
time=2025-08-10T22:05:31.697-07:00 level=INFO source=server.go:658 msg="llama runner started in 48.74 seconds"
[GIN] 2025/08/10 - 22:08:25 | 200 |    1.343239ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:11:25 | 200 |     1.52774ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:14:25 | 200 |    1.463208ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:17:25 | 200 |    1.253713ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:20:25 | 200 |    1.233526ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:23:40 | 200 |     1.79097ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:26:48 | 200 |     1.44906ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:30:05 | 200 |    1.363783ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:33:12 | 200 |    1.448384ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:33:48 | 200 |         29m6s |   192.168.0.165 | POST     "/api/chat"
[GIN] 2025/08/10 - 22:33:48 | 200 |     576.656µs |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:35:14 | 200 |    1.588057ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:36:01 | 200 |    2.198159ms |   192.168.0.165 | GET      "/api/tags"
time=2025-08-10T22:36:06.382-07:00 level=ERROR source=gpu_rpc.go:59 msg="failed to fetch RPC server reply size of RPC_CMD_HELLO" err="read tcp 192.168.0.157:44744->192.168.0.140:50052: i/o timeout"
time=2025-08-10T22:36:06.383-07:00 level=WARN source=gpu_rpc.go:177 msg="unable to connect to endpoint" endpoint=192.168.0.140:50052
time=2025-08-10T22:36:11.384-07:00 level=ERROR source=gpu_rpc.go:59 msg="failed to fetch RPC server reply size of RPC_CMD_HELLO" err="read tcp 192.168.0.157:37586->192.168.0.114:50052: i/o timeout"
time=2025-08-10T22:36:11.384-07:00 level=WARN source=gpu_rpc.go:177 msg="unable to connect to endpoint" endpoint=192.168.0.114:50052
time=2025-08-10T22:36:11.491-07:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-<ID> library=cuda total="3.9 GiB" available="3.9 GiB"
[GIN] 2025/08/10 - 22:36:12 | 200 |    2.172621ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:39:12 | 200 |    1.937388ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:42:12 | 200 |    1.094088ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:45:12 | 200 |    1.538605ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:45:31 | 200 |        11m42s |   192.168.0.165 | POST     "/api/chat"
time=2025-08-10T22:45:36.336-07:00 level=WARN source=sched.go:697 msg="gpu VRAM usage didn't recover within timeout" seconds=5.043045807 runner.size="5.6 GiB" runner.vram="5.6 GiB" runner.parallel=1 runner.pid=1567 runner.model=/home/llm/
.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff
time=2025-08-10T22:45:36.555-07:00 level=INFO source=server.go:135 msg="system memory" total="15.6 GiB" free="14.6 GiB" free_swap="976.0 MiB"
time=2025-08-10T22:45:36.555-07:00 level=INFO source=server.go:187 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=0 layers.split="" memory.available="[3.9 GiB 3.8 GiB 2.9 GiB]" memory.gpu_overhead="0 B" memory
.required.full="12.2 GiB" memory.required.partial="0 B" memory.required.kv="492.0 MiB" memory.required.allocations="[0 B 0 B 0 B]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" m
emory.graph.full="4.0 GiB" memory.graph.partial="4.0 GiB"
time=2025-08-10T22:45:36.586-07:00 level=WARN source=sched.go:697 msg="gpu VRAM usage didn't recover within timeout" seconds=5.293398349 runner.size="5.6 GiB" runner.vram="5.6 GiB" runner.parallel=1 runner.pid=1567 runner.model=/home/llm/
.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff
time=2025-08-10T22:45:36.642-07:00 level=INFO source=server.go:459 msg="starting llama server" cmd="/home/llm/.cache/go-build/02/02d222919dcb78c0bc6a0c058acebcc67c58bf245a3ca25fb7d48db2b2b875cb-d/ollama runner --ollama-engine --model /hom
e/llm/.ollama/models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 16384 --batch-size 512 --rpc 192.168.0.140:50052,192.168.0.114:50052 --threads 4 --no-mmap --parallel 1 --port 34549"
time=2025-08-10T22:45:36.642-07:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-08-10T22:45:36.642-07:00 level=INFO source=server.go:619 msg="waiting for llama runner to start responding"
time=2025-08-10T22:45:36.643-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-10T22:45:36.660-07:00 level=INFO source=runner.go:935 msg="starting ollama engine"
time=2025-08-10T22:45:36.661-07:00 level=INFO source=runner.go:994 msg="Server listening on 127.0.0.1:34549"
time=2025-08-10T22:45:36.727-07:00 level=INFO source=ggml.go:122 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce GTX 745, compute capability 5.0, VMM: yes
load_backend: loaded CUDA backend from /home/llm/llm-server/ollama/rpc/ollama-rpc-GH-10844/build/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /home/llm/llm-server/ollama/rpc/ollama-rpc-GH-10844/build/lib/ollama/libggml-cpu-haswell.so
time=2025-08-10T22:45:36.768-07:00 level=INFO source=ggml.go:105 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500 CUDA.0.USE_GRAPHS=1
 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-08-10T22:45:36.800-07:00 level=INFO source=ggml.go:390 msg="offloading 0 repeating layers to GPU"
time=2025-08-10T22:45:36.800-07:00 level=INFO source=ggml.go:394 msg="offloading output layer to CPU"
time=2025-08-10T22:45:36.800-07:00 level=INFO source=ggml.go:401 msg="offloaded 0/25 layers to GPU"
time=2025-08-10T22:45:36.800-07:00 level=INFO source=ggml.go:404 msg="model weights" buffer=CPU size="12.8 GiB"
time=2025-08-10T22:45:36.836-07:00 level=WARN source=sched.go:697 msg="gpu VRAM usage didn't recover within timeout" seconds=5.542821631 runner.size="5.6 GiB" runner.vram="5.6 GiB" runner.parallel=1 runner.pid=1567 runner.model=/home/llm/
.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff
time=2025-08-10T22:45:36.893-07:00 level=INFO source=ggml.go:707 msg="compute graph" backend=RPC[192.168.0.140:50052] buffer_type=RPC[192.168.0.140:50052] size="0 B"
time=2025-08-10T22:45:36.894-07:00 level=INFO source=ggml.go:707 msg="compute graph" backend=RPC[192.168.0.114:50052] buffer_type=RPC[192.168.0.114:50052] size="0 B"
time=2025-08-10T22:45:36.894-07:00 level=INFO source=ggml.go:707 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="0 B"
time=2025-08-10T22:45:36.894-07:00 level=INFO source=ggml.go:707 msg="compute graph" backend=CPU buffer_type=CPU size="4.0 GiB"
time=2025-08-10T22:45:36.894-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-10T22:46:01.032-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-10T22:46:01.471-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-10T22:46:06.569-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-10T22:46:06.820-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-10T22:46:11.161-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-10T22:46:16.013-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-10T22:46:16.466-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-10T22:46:16.717-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-10T22:46:24.671-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-10T22:46:24.922-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-10T22:46:28.749-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-10T22:46:29.451-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-10T22:46:47.423-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-10T22:46:47.674-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-10T22:46:55.230-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-10T22:46:55.482-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-10T22:46:58.984-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-10T22:46:59.235-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-10T22:47:00.707-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-10T22:47:00.958-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-10T22:47:01.750-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-10T22:47:06.256-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-10T22:47:07.799-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-10T22:47:09.850-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-10T22:47:11.266-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-10T22:47:12.562-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-10T22:47:14.123-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-10T22:47:18.283-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-10T22:47:19.803-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-10T22:47:20.054-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-10T22:47:26.403-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-10T22:47:31.365-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-10T22:47:32.599-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-10T22:48:48.159-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-10T22:48:49.715-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-10T22:48:50.018-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-10T22:48:53.925-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
[GIN] 2025/08/10 - 22:48:53 | 200 |  5.731828325s |   192.168.0.165 | GET      "/api/tags"
time=2025-08-10T22:48:56.175-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-10T22:48:56.626-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-10T22:48:57.329-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-10T22:49:16.606-07:00 level=INFO source=server.go:658 msg="llama runner started in 219.96 seconds"
[GIN] 2025/08/10 - 22:51:14 | 200 |     575.519µs |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:54:34 | 200 |     711.531µs |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:57:42 | 200 |     570.736µs |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:57:48 | 200 |        21m46s |   192.168.0.165 | POST     "/api/chat"
[GIN] 2025/08/10 - 22:57:48 | 200 |     428.989µs |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:59:11 | 200 |     630.518µs |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:00:08 | 200 |     742.401µs |   192.168.0.165 | GET      "/api/tags"
time=2025-08-10T23:00:11.116-07:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-<ID> library=cuda total="3.9 GiB" available="3.9 GiB"
time=2025-08-10T23:00:11.116-07:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=192.168.0.140:50052 library=rpc total="3.8 GiB" available="3.8 GiB"
time=2025-08-10T23:00:11.116-07:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=192.168.0.114:50052 library=rpc total="2.9 GiB" available="2.9 GiB"
[GIN] 2025/08/10 - 23:00:42 | 200 |     575.841µs |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:01:42 | 200 |         3m54s |   192.168.0.165 | POST     "/api/chat"
time=2025-08-10T23:01:44.361-07:00 level=INFO source=server.go:135 msg="system memory" total="15.6 GiB" free="14.7 GiB" free_swap="868.7 MiB"
time=2025-08-10T23:01:44.362-07:00 level=INFO source=server.go:187 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=28 layers.split=9,12,7 memory.available="[3.9 GiB 3.8 GiB 2.9 GiB]" memory.gpu_overhead="0 B" m
emory.required.full="12.3 GiB" memory.required.partial="10.4 GiB" memory.required.kv="2.2 GiB" memory.required.allocations="[3.7 GiB 3.7 GiB 2.9 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrep
eating="486.9 MiB" memory.graph.full="1.5 GiB" memory.graph.partial="1.5 GiB"
llama_model_loader: loaded meta data with 32 key-value pairs and 399 tensors from /home/llm/.ollama/models/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 0528 Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-0528-Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = mit
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 131072
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                    qwen3.rope.scaling.type str              = yarn
llama_model_loader: - kv  17:                  qwen3.rope.scaling.factor f32              = 4.000000
llama_model_loader: - kv  18: qwen3.rope.scaling.original_context_length u32              = 32768
llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,151387]  = ["   ", "     ", "i n", "  t",...
llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 151645
llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  30:               general.quantization_version u32              = 2
llama_model_loader: - kv  31:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW)
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 28
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = DeepSeek R1 0528 Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<\ÿbeginofsentence\ÿ>'
print_info: EOS token        = 151645 '<\ÿendofsentence\ÿ>'
print_info: EOT token        = 151645 '<\ÿendofsentence\ÿ>'
print_info: PAD token        = 151645 '<\ÿendofsentence\ÿ>'
print_info: LF token         = 198 '
'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151645 '<\ÿendofsentence\ÿ>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-10T23:01:45.137-07:00 level=INFO source=server.go:459 msg="starting llama server" cmd="/home/llm/.cache/go-build/02/02d222919dcb78c0bc6a0c058acebcc67c58bf245a3ca25fb7d48db2b2b875cb-d/ollama runner --model /home/llm/.ollama/mo
dels/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d --ctx-size 16384 --batch-size 512 --n-gpu-layers 28 --rpc 192.168.0.140:50052,192.168.0.114:50052 --threads 4 --no-mmap --parallel 1 --tensor-split 9,12,7
--port 44977"
time=2025-08-10T23:01:45.137-07:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-08-10T23:01:45.137-07:00 level=INFO source=server.go:619 msg="waiting for llama runner to start responding"
time=2025-08-10T23:01:45.169-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-10T23:01:45.955-07:00 level=INFO source=runner.go:816 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce GTX 745, compute capability 5.0, VMM: yes
load_backend: loaded CUDA backend from /home/llm/llm-server/ollama/rpc/ollama-rpc-GH-10844/build/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /home/llm/llm-server/ollama/rpc/ollama-rpc-GH-10844/build/lib/ollama/libggml-cpu-haswell.so
time=2025-08-10T23:01:49.393-07:00 level=INFO source=ggml.go:105 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500 CUDA.0.USE_GRAPHS=1
 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
llama_model_load_from_file_impl: using device RPC[192.168.0.140:50052] (RPC[192.168.0.140:50052]) - 3848 MiB free
llama_model_load_from_file_impl: using device RPC[192.168.0.114:50052] (RPC[192.168.0.114:50052]) - 2968 MiB free
time=2025-08-10T23:01:49.520-07:00 level=INFO source=runner.go:876 msg="Server listening on 127.0.0.1:44977"
time=2025-08-10T23:01:49.520-07:00 level=INFO source=server.go:653 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce GTX 745) - 4005 MiB free
llama_model_loader: loaded meta data with 32 key-value pairs and 399 tensors from /home/llm/.ollama/models/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 0528 Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-0528-Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = mit
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 131072
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                    qwen3.rope.scaling.type str              = yarn
llama_model_loader: - kv  17:                  qwen3.rope.scaling.factor f32              = 4.000000
llama_model_loader: - kv  18: qwen3.rope.scaling.original_context_length u32              = 32768
llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,151387]  = ["   ", "     ", "i n", "  t",...
llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 151645
llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  30:               general.quantization_version u32              = 2
llama_model_loader: - kv  31:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW)
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 28
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = yarn
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 0.25
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = DeepSeek R1 0528 Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<\ÿbeginofsentence\ÿ>'
print_info: EOS token        = 151645 '<\ÿendofsentence\ÿ>'
print_info: EOT token        = 151645 '<\ÿendofsentence\ÿ>'
print_info: PAD token        = 151645 '<\ÿendofsentence\ÿ>'
print_info: LF token         = 198 '
'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151645 '<\ÿendofsentence\ÿ>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloaded 28/37 layers to GPU
load_tensors:    CUDA_Host model buffer size =  1423.00 MiB
load_tensors:        CUDA0 model buffer size =   839.23 MiB
load_tensors: RPC[192.168.0.140:50052] model buffer size =  1020.67 MiB
load_tensors: RPC[192.168.0.114:50052] model buffer size =  1360.89 MiB
load_tensors:          CPU model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 16384
llama_context: n_ctx_per_seq = 16384
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 0.25
llama_context: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 16384, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   448.00 MiB
llama_kv_cache_unified: RPC[192.168.0.140:50052] KV buffer size =   576.00 MiB
llama_kv_cache_unified: RPC[192.168.0.114:50052] KV buffer size =   768.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =   512.00 MiB
llama_kv_cache_unified: KV self size  = 2304.00 MiB, K (f16): 1152.00 MiB, V (f16): 1152.00 MiB
llama_context: RPC[192.168.0.140:50052] compute buffer size =  1088.00 MiB
llama_context: RPC[192.168.0.114:50052] compute buffer size =  1088.00 MiB
llama_context:      CUDA0 compute buffer size =  1176.00 MiB
llama_context:        CPU compute buffer size =    40.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 111 (with bs=512), 21 (with bs=1)
time=2025-08-10T23:03:07.409-07:00 level=INFO source=server.go:658 msg="llama runner started in 82.27 seconds"
[GIN] 2025/08/10 - 23:03:42 | 200 |    1.379227ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:06:42 | 200 |     1.93637ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:09:42 | 200 |    1.624713ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:12:53 | 200 |    1.649121ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:16:11 | 200 |    1.497309ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:19:16 | 200 |    1.635962ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:22:27 | 200 |    1.451421ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:25:29 | 200 |    1.451194ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:28:47 | 200 |    1.626204ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:31:48 | 200 |    2.341172ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:34:49 | 200 |    1.334881ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:34:57 | 200 |     1.26154ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:37:20 | 200 |    1.732271ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:37:49 | 200 |    1.347358ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:40:49 | 200 |    1.652315ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:43:49 | 200 |    1.677496ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:45:21 | 200 |        45m13s |   192.168.0.165 | POST     "/api/chat"
[GIN] 2025/08/10 - 23:45:21 | 200 |     429.514µs |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:46:49 | 200 |    1.227912ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:49:49 | 200 |    1.512503ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:51:15 | 200 |    2.217063ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:52:49 | 200 |    1.524185ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:55:49 | 200 |    2.088895ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:58:49 | 200 |    1.582068ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:59:20 | 200 |    1.508063ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/10 - 23:59:47 | 200 |    1.440028ms |   192.168.0.165 | GET      "/api/tags"
[GIN] 2025/08/11 - 00:00:34 | 200 |     827.458µs |   192.168.0.165 | GET      "/api/tags"

