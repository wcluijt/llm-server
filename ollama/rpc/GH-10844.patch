From f27da2fe5f4d45d9be13d29dd2b3f2b2092fb103 Mon Sep 17 00:00:00 2001
From: Willem Luijt <wcluijt@gmail.com>
Date: Sat, 2 Aug 2025 19:58:58 -0700
Subject: [PATCH] rpc-GH-10844: discover/gpu.go: Add Support for Distributed
 Inferencing (continued)

GitHub Pull Request: https://github.com/ollama/ollama/pull/10844

Original Code: https://github.com/gkpln3/ollama/tree/feat/rpc (commit 84aa6d0a82ff6fb7d68287b7031846d97e2127b5)

Code applied on latest ollama git main branch (commit 4183bb0574a28b73276efef944107d0c45d79c95) with:

git clone https://github.com/ollama/ollama.git ollama-rpc-GH-10844;
cd ./ollama-rpc-GH-10844;
git checkout main;
git pull;
git checkout -b rpc-GH-10844;
curl -LO https://github.com/ollama/ollama/pull/10844.diff;
git apply --3way 10844.diff;

Conflicts in llm/server.go file resolved to include patch code above conflicted area.
---
 CMakeLists.txt                                |    4 +
 api/types.go                                  |    2 +
 cmd/cmd.go                                    |   14 +
 cmd/rpc_server.go                             |   48 +
 discover/gpu.go                               |    1 +
 discover/gpu_darwin.go                        |    2 +-
 discover/gpu_rpc.go                           |  185 ++
 discover/types.go                             |    9 +
 docs/api.md                                   |    3 +-
 docs/distributed_inferencing.md               |   37 +
 envconfig/config.go                           |    3 +
 llama/llama.go                                |   11 +
 ...c-and-free-using-the-same-compiler.patched |    0
 llama/patches/0002-pretokenizer.patched       |    0
 llama/patches/0003-embeddings.patched         |    0
 llama/patches/0004-clip-unicode.patched       |    0
 llama/patches/0005-solar-pro.patched          |    0
 llama/patches/0006-add-mllama-support.patched |    0
 llama/patches/0007-add-unpad-operator.patched |    0
 .../0008-fix-deepseek-deseret-regex.patched   |    0
 ...ain-ordering-for-rules-for-grammar.patched |    0
 ...ure-KV-cache-is-fully-defragmented.patched |    0
 .../0011-sort-devices-by-score.patched        |    0
 ...rget-ggml-cpu-for-all-cpu-variants.patched |    0
 llama/patches/0013-remove-amx.patched         |    0
 .../0014-fix-string-arr-kv-loading.patched    |    0
 .../patches/0015-ollama-debug-tensor.patched  |    0
 .../0016-add-model-quantizations.patched      |    0
 ...d-ollama-vocab-for-grammar-support.patched |    0
 ...fail-when-tensor-data-changes-1322.patched |    0
 llm/server.go                                 |   25 +-
 ml/backend.go                                 |    3 +
 ml/backend/ggml/ggml.go                       |    8 +
 ml/backend/ggml/ggml/.rsync-filter            |    1 +
 ml/backend/ggml/ggml/include/rpc-server.h     |   10 +
 .../ggml/ggml/src/ggml-rpc/CMakeLists.txt     |   10 +
 .../ggml/ggml/src/ggml-rpc/ggml-rpc.cpp       | 1824 +++++++++++++++++
 .../ggml/ggml/src/ggml-rpc/rpc-server.cpp     |  326 +++
 ml/backend/ggml/ggml/src/ggml-rpc/rpc.go      |    6 +
 ml/backend/ggml/ggml/src/ggml.go              |    3 +-
 ml/backend/ggml/utils.cpp                     |   48 +
 ml/backend/ggml/utils.h                       |   16 +
 runner/llamarunner/runner.go                  |    2 +
 runner/ollamarunner/runner.go                 |    2 +
 server/routes.go                              |    3 +
 server/sched.go                               |   18 +-
 46 files changed, 2616 insertions(+), 8 deletions(-)
 create mode 100644 cmd/rpc_server.go
 create mode 100644 discover/gpu_rpc.go
 create mode 100644 docs/distributed_inferencing.md
 create mode 100644 llama/patches/0001-ggml-backend-malloc-and-free-using-the-same-compiler.patched
 create mode 100644 llama/patches/0002-pretokenizer.patched
 create mode 100644 llama/patches/0003-embeddings.patched
 create mode 100644 llama/patches/0004-clip-unicode.patched
 create mode 100644 llama/patches/0005-solar-pro.patched
 create mode 100644 llama/patches/0006-add-mllama-support.patched
 create mode 100644 llama/patches/0007-add-unpad-operator.patched
 create mode 100644 llama/patches/0008-fix-deepseek-deseret-regex.patched
 create mode 100644 llama/patches/0009-maintain-ordering-for-rules-for-grammar.patched
 create mode 100644 llama/patches/0010-ensure-KV-cache-is-fully-defragmented.patched
 create mode 100644 llama/patches/0011-sort-devices-by-score.patched
 create mode 100644 llama/patches/0012-add-phony-target-ggml-cpu-for-all-cpu-variants.patched
 create mode 100644 llama/patches/0013-remove-amx.patched
 create mode 100644 llama/patches/0014-fix-string-arr-kv-loading.patched
 create mode 100644 llama/patches/0015-ollama-debug-tensor.patched
 create mode 100644 llama/patches/0016-add-model-quantizations.patched
 create mode 100644 llama/patches/0017-add-ollama-vocab-for-grammar-support.patched
 create mode 100644 llama/patches/0018-ggml-Don-t-assert-fail-when-tensor-data-changes-1322.patched
 create mode 100644 ml/backend/ggml/ggml/include/rpc-server.h
 create mode 100644 ml/backend/ggml/ggml/src/ggml-rpc/CMakeLists.txt
 create mode 100644 ml/backend/ggml/ggml/src/ggml-rpc/ggml-rpc.cpp
 create mode 100644 ml/backend/ggml/ggml/src/ggml-rpc/rpc-server.cpp
 create mode 100644 ml/backend/ggml/ggml/src/ggml-rpc/rpc.go
 create mode 100644 ml/backend/ggml/utils.cpp
 create mode 100644 ml/backend/ggml/utils.h

diff --git a/CMakeLists.txt b/CMakeLists.txt
index b3b5438a..57ea36e6 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -57,6 +57,10 @@ set(GGML_CPU ON)
 add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/ml/backend/ggml/ggml/src)
 set_property(TARGET ggml PROPERTY EXCLUDE_FROM_ALL TRUE)
 
+set(GGML_RPC ON)
+# Not sure if this is needed as it still works without it
+add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/ml/backend/ggml/ggml/src/ggml-rpc)
+
 get_target_property(CPU_VARIANTS ggml-cpu MANUALLY_ADDED_DEPENDENCIES)
 if(NOT CPU_VARIANTS)
     set(CPU_VARIANTS "ggml-cpu")
diff --git a/api/types.go b/api/types.go
index 699dba42..7f7468ca 100644
--- a/api/types.go
+++ b/api/types.go
@@ -296,6 +296,8 @@ type Runner struct {
 	MainGPU   int   `json:"main_gpu,omitempty"`
 	UseMMap   *bool `json:"use_mmap,omitempty"`
 	NumThread int   `json:"num_thread,omitempty"`
+	// RPCServers sepecifies a comma seperated list of RPC servers to use for inteference.
+	RPCServers string `json:"rpc_servers,omitempty"`
 }
 
 // EmbedRequest is the request passed to [Client.Embed].
diff --git a/cmd/cmd.go b/cmd/cmd.go
index 1d1d116b..f3a44a82 100644
--- a/cmd/cmd.go
+++ b/cmd/cmd.go
@@ -1544,6 +1544,17 @@ func NewCLI() *cobra.Command {
 		_ = runner.Execute(args[1:])
 	})
 
+	rpcCmd := &cobra.Command{
+		Use:   "rpc",
+		Short: "Start an RPC server for distributed inference",
+		RunE:  rpcServerRun,
+		// PersistentPreRunE from rootCmd will apply
+	}
+
+	rpcCmd.Flags().String("host", "0.0.0.0", "Host address for the RPC server")
+	rpcCmd.Flags().Int("port", 50052, "Port for the RPC server")
+	rpcCmd.Flags().String("device", "", "Device to use (use --device list to see all)")
+
 	envVars := envconfig.AsMap()
 
 	envs := []envconfig.EnvVar{envVars["OLLAMA_HOST"]}
@@ -1560,6 +1571,7 @@ func NewCLI() *cobra.Command {
 		copyCmd,
 		deleteCmd,
 		serveCmd,
+		rpcCmd,
 	} {
 		switch cmd {
 		case runCmd:
@@ -1581,6 +1593,7 @@ func NewCLI() *cobra.Command {
 				envVars["OLLAMA_LLM_LIBRARY"],
 				envVars["OLLAMA_GPU_OVERHEAD"],
 				envVars["OLLAMA_LOAD_TIMEOUT"],
+				envVars["OLLAMA_RPC_SERVERS"],
 			})
 		default:
 			appendEnvDocs(cmd, envs)
@@ -1600,6 +1613,7 @@ func NewCLI() *cobra.Command {
 		copyCmd,
 		deleteCmd,
 		runnerCmd,
+		rpcCmd,
 	)
 
 	return rootCmd
diff --git a/cmd/rpc_server.go b/cmd/rpc_server.go
new file mode 100644
index 00000000..40a7ffb7
--- /dev/null
+++ b/cmd/rpc_server.go
@@ -0,0 +1,48 @@
+package cmd
+
+/*
+#cgo CPPFLAGS: -I${SRCDIR}/../ml/backend/ggml/ggml/include
+#include "rpc-server.h"
+*/
+import "C"
+
+import (
+	"fmt"
+	"log"
+	"os"
+	"os/signal"
+	"syscall"
+
+	ggml "github.com/ollama/ollama/ml/backend/ggml/ggml/src"
+	"github.com/spf13/cobra"
+)
+
+func rpcServerRun(cmd *cobra.Command, args []string) error {
+	rpcHost, _ := cmd.Flags().GetString("host")
+	rpcPort, _ := cmd.Flags().GetInt("port")
+	device, _ := cmd.Flags().GetString("device")
+
+	endpoint := fmt.Sprintf("%s:%d", rpcHost, rpcPort)
+	log.Printf("Starting RPC server on %s", endpoint)
+
+	ggml.OnceLoad()
+
+	// Run ggml_backend_rpc_start_server in a goroutine
+	// so we can handle signals and potentially stop it later if needed.
+	// Note: ggml_backend_rpc_start_server is likely a blocking call.
+	go func() {
+		C.run_rpc_server(C.CString(rpcHost), C.int(rpcPort), C.CString(device))
+	}()
+
+	log.Printf("RPC server started on %s. Press Ctrl+C to exit.", endpoint)
+
+	// Wait for an interrupt signal
+	sigChan := make(chan os.Signal, 1)
+	signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)
+	<-sigChan
+
+	log.Println("Shutting down RPC server...")
+	// Add any cleanup logic here if ggml_backend_rpc_start_server has a corresponding stop function.
+	// For now, we assume exiting the program stops the server.
+	return nil
+}
diff --git a/discover/gpu.go b/discover/gpu.go
index 15bad446..6695aa84 100644
--- a/discover/gpu.go
+++ b/discover/gpu.go
@@ -406,6 +406,7 @@ func GetGPUInfo() GpuInfoList {
 			cpus[0].FreeSwap = mem.FreeSwap
 		}
 
+		// CUDA GPU
 		var memInfo C.mem_info_t
 		if cHandles == nil && len(cudaGPUs) > 0 {
 			cHandles = initCudaHandles()
diff --git a/discover/gpu_darwin.go b/discover/gpu_darwin.go
index dd5bf6e2..7e67fb70 100644
--- a/discover/gpu_darwin.go
+++ b/discover/gpu_darwin.go
@@ -39,8 +39,8 @@ func GetGPUInfo() GpuInfoList {
 
 	// TODO is there a way to gather actual allocated video memory? (currentAllocatedSize doesn't work)
 	info.FreeMemory = info.TotalMemory
-
 	info.MinimumMemory = metalMinimumMemory
+
 	return []GpuInfo{info}
 }
 
diff --git a/discover/gpu_rpc.go b/discover/gpu_rpc.go
new file mode 100644
index 00000000..88282dcb
--- /dev/null
+++ b/discover/gpu_rpc.go
@@ -0,0 +1,185 @@
+package discover
+
+import (
+	"encoding/binary"
+	"log/slog"
+	"net"
+	"strconv"
+	"strings"
+	"time"
+)
+
+// RPCServerMemory checks and total and free memory in bytes of a given RPC
+// endpoint.
+//
+// If the RPC endpoint given is unavailable (unable to connect), the total and
+// free memory returned would be 0.
+func getRPCServerMemory(endpoint string) RPCServerMemoryResult {
+	// Setting timeout to 5 seconds
+	var deadLine time.Time
+	timeout := time.Duration(5 * 1000 * 1000 * 1000)
+
+	slog.Debug("getting memory for RPC server", "endpoint", endpoint)
+	// Creating RPC client
+	client, err := net.DialTimeout("tcp", endpoint, timeout)
+	if err != nil {
+		return RPCServerMemoryResult{}
+	}
+	defer client.Close()
+	slog.Debug("connection established with server", "endpoint", endpoint)
+
+	// Sending RPC_CMD_HELLO command
+	// RPC Command (1 byte)
+	deadLine = time.Now().Add(timeout)
+	client.SetDeadline(deadLine)
+	_, err = client.Write([]byte{14})
+	if err != nil {
+		slog.Error("failed to send RPC_CMD_HELLO command to RPC server", "err", err)
+		return RPCServerMemoryResult{}
+	}
+	slog.Debug("successfully sent RPC_CMD_HELLO command")
+	// Input Size (8 bytes)
+	deadLine = time.Now().Add(timeout)
+	client.SetDeadline(deadLine)
+	helloSize := [8]byte{}
+	_, err = client.Write(helloSize[:])
+	if err != nil {
+		slog.Error("failed to send input size of RPC_CMD_HELLO command to RPC server", "err", err)
+		return RPCServerMemoryResult{}
+	}
+	slog.Debug("successfully sent input size of RPC_CMD_HELLO command")
+
+	// Retrieving results for RPC_CMD_HELLO command
+	// Getting reply size (8 bytes)
+	deadLine = time.Now().Add(timeout)
+	client.SetDeadline(deadLine)
+	helloReply := [8]byte{}
+	_, err = client.Read(helloReply[:])
+	if err != nil {
+		slog.Error("failed to fetch RPC server reply size of RPC_CMD_HELLO", "err", err)
+		return RPCServerMemoryResult{}
+	}
+	helloReplySize := binary.LittleEndian.Uint64(helloReply[:])
+	slog.Debug("RPC_CMD_HELLO reply size", "size", helloReplySize)
+	// Reply size should be 3 according to spec
+	if helloReplySize != 3 {
+		slog.Error("invalid reply size for RPC_CMD_HELLO")
+		return RPCServerMemoryResult{}
+	}
+	// Getting main reply
+	// The version of the RPC server (3 bytes)
+	deadLine = time.Now().Add(timeout)
+	client.SetDeadline(deadLine)
+	serverVersion := [3]byte{}
+	_, err = client.Read(serverVersion[:])
+	if err != nil {
+		slog.Error("failed to fetch RPC server version from RPC_CMD_HELLO", "err", err)
+		return RPCServerMemoryResult{}
+	}
+	slog.Debug("RPC_CMD_HELLO reply", "major", serverVersion[0], "minor", serverVersion[1], "patch", serverVersion[2])
+
+	// Sending RPC_CMD_GET_DEVICE_MEMORY command
+	// RPC Command (1 byte)
+	deadLine = time.Now().Add(timeout)
+	client.SetDeadline(deadLine)
+	_, err = client.Write([]byte{11})
+	if err != nil {
+		slog.Error("failed to send RPC_CMD_GET_DEVICE_MEMORY command to RPC server", "err", err)
+		return RPCServerMemoryResult{}
+	}
+	slog.Debug("successfully sent RPC_CMD_GET_DEVICE_MEMORY command")
+	// Input Size (8 bytes)
+	deadLine = time.Now().Add(timeout)
+	client.SetDeadline(deadLine)
+	size := [8]byte{}
+	_, err = client.Write(size[:])
+	if err != nil {
+		slog.Error("failed to send input size of RPC_CMD_GET_DEVICE_MEMORY command to RPC server", "err", err)
+		return RPCServerMemoryResult{}
+	}
+	slog.Debug("successfully sent input size RPC_CMD_GET_DEVICE_MEMORY command")
+
+	// Retrieving results for RPC_CMD_GET_DEVICE_MEMORY command
+	// Getting reply size (8 bytes)
+	deadLine = time.Now().Add(timeout)
+	client.SetDeadline(deadLine)
+	reply := [8]byte{}
+	_, err = client.Read(reply[:])
+	if err != nil {
+		slog.Error("failed to fetch RPC server reply size of RPC_CMD_GET_DEVICE_MEMORY", "err", err)
+		return RPCServerMemoryResult{}
+	}
+	reply_size := binary.LittleEndian.Uint64(reply[:])
+	// Reply size should be 16 according to spec
+	if reply_size != 16 {
+		slog.Error("invalid reply size received from RPC server")
+		return RPCServerMemoryResult{}
+	}
+	// Getting main reply
+	// The free memory of the RPC server (8 bytes)
+	deadLine = time.Now().Add(timeout)
+	client.SetDeadline(deadLine)
+	freeMem := [8]byte{}
+	_, err = client.Read(freeMem[:])
+	if err != nil {
+		return RPCServerMemoryResult{}
+	}
+	// The total memory of the RPC server (8 bytes)
+	deadLine = time.Now().Add(timeout)
+	client.SetDeadline(deadLine)
+	totalMem := [8]byte{}
+	_, err = client.Read(totalMem[:])
+	if err != nil {
+		return RPCServerMemoryResult{}
+	}
+
+	return RPCServerMemoryResult{
+		FreeMem:  binary.LittleEndian.Uint64(freeMem[:]),
+		TotalMem: binary.LittleEndian.Uint64(totalMem[:]),
+	}
+}
+
+// Find valid RPC servers from a comma seperated list of endpoints.
+func GetRPCServers(endpoints string) GpuInfoList {
+	slog.Debug("finding valid rpc servers", "endpoints", endpoints)
+	rpcServersList := strings.Split(endpoints, ",")
+	var validServers GpuInfoList
+	for _, server := range rpcServersList {
+		// No servers given
+		if server == "" {
+			break
+		}
+
+		// Getting information
+		info := getRPCServerMemory(server)
+		serverAddress := strings.Split(server, ":")
+		// We got an invalid server address
+		if len(serverAddress) != 2 {
+			slog.Warn("invalid RPC endpoint server address", "endpoint", server)
+			continue
+		}
+		// Invalid port number
+		_, err := strconv.ParseUint(serverAddress[len(serverAddress)-1], 10, 16)
+		if err != nil {
+			slog.Warn("invalid RPC endpoint port number", "endpoint", server)
+			continue
+		}
+
+		serverInfo := GpuInfo{
+			ID:      server,
+			Library: "rpc",
+		}
+
+		serverInfo.TotalMemory = info.TotalMem
+		serverInfo.FreeMemory = info.FreeMem
+
+		if serverInfo.TotalMemory == 0 && serverInfo.FreeMemory == 0 {
+			slog.Warn("unable to connect to endpoint", "endpoint", server)
+		} else {
+			slog.Debug("found RPC server", "info", serverInfo)
+			validServers = append(validServers, serverInfo)
+		}
+	}
+
+	return validServers
+}
diff --git a/discover/types.go b/discover/types.go
index c5212d94..40339dd7 100644
--- a/discover/types.go
+++ b/discover/types.go
@@ -69,6 +69,15 @@ type CPU struct {
 	ThreadCount         int
 }
 
+// RPCServerMemoryResult represents the return value of the RPCServerMemory
+// function.
+type RPCServerMemoryResult struct {
+	// The amount of free memory in bytes available in the RPC endpoint.
+	FreeMem uint64
+	// The total memory available in bytes on the RPC endpoint.
+	TotalMem uint64
+}
+
 type CudaGPUInfo struct {
 	GpuInfo
 	OSOverhead   uint64 // Memory overhead between the driver library and management library
diff --git a/docs/api.md b/docs/api.md
index 683db357..38e7f68b 100644
--- a/docs/api.md
+++ b/docs/api.md
@@ -403,7 +403,8 @@ curl http://localhost:11434/api/generate -d '{
     "num_gpu": 1,
     "main_gpu": 0,
     "use_mmap": true,
-    "num_thread": 8
+    "num_thread": 8,
+    "rpc_servers": "127.0.0.1:50052"
   }
 }'
 ```
diff --git a/docs/distributed_inferencing.md b/docs/distributed_inferencing.md
new file mode 100644
index 00000000..960f8146
--- /dev/null
+++ b/docs/distributed_inferencing.md
@@ -0,0 +1,37 @@
+# Distributed Inferencing
+
+Ollama supports distributed inference via llama.cpp RPC servers, enabling larger models to run across multiple machines within the same network.
+
+Note: Model loading can be very slow over a slow network.
+
+# Getting Started
+
+Start the RPC server on each machine:
+```
+ollama rpc [--host 0.0.0.0] [--port 50052]
+```
+
+## Configure with Environment Variable
+
+Specify the RPC servers using the OLLAMA_RPC_SERVERS environment variable. Provide a comma-separated list of `host:port` pairs:
+
+```sh
+OLLAMA_RPC_SERVERS="127.0.0.1:50052,192.168.0.69:50053" ollama serve
+```
+
+## Override with Request Options
+
+You can override the RPC servers per request using the `rpc_servers `option in the API call:
+
+```sh
+curl http://localhost:11434/api/generate --json '{
+  "model": "llama3.1",
+  "prompt": "hello",
+  "stream": false,
+  "options": {
+    "rpc_servers": "127.0.0.1:50053"
+  }
+}'
+```
+
+Ollama will use the RPC server `127.0.0.1:50053` instead of the servers set by `OLLAMA_RPC_SERVERS` environment variable.
diff --git a/envconfig/config.go b/envconfig/config.go
index 7fc01887..275702a4 100644
--- a/envconfig/config.go
+++ b/envconfig/config.go
@@ -179,6 +179,8 @@ var (
 	IntelGPU = Bool("OLLAMA_INTEL_GPU")
 	// MultiUserCache optimizes prompt caching for multi-user scenarios
 	MultiUserCache = Bool("OLLAMA_MULTIUSER_CACHE")
+	// RPCServers specifies a comma separated list of RPC servers to use for inference.
+	RPCServers = String("OLLAMA_RPC_SERVERS")
 	// Enable the new Ollama engine
 	NewEngine = Bool("OLLAMA_NEW_ENGINE")
 	// ContextLength sets the default context length
@@ -268,6 +270,7 @@ func AsMap() map[string]EnvVar {
 		"OLLAMA_ORIGINS":           {"OLLAMA_ORIGINS", AllowedOrigins(), "A comma separated list of allowed origins"},
 		"OLLAMA_SCHED_SPREAD":      {"OLLAMA_SCHED_SPREAD", SchedSpread(), "Always schedule model across all GPUs"},
 		"OLLAMA_MULTIUSER_CACHE":   {"OLLAMA_MULTIUSER_CACHE", MultiUserCache(), "Optimize prompt caching for multi-user scenarios"},
+		"OLLAMA_RPC_SERVERS":       {"OLLAMA_RPC_SERVERS", RPCServers(), "A comma separated list of RPC server to disribute models to"},
 		"OLLAMA_CONTEXT_LENGTH":    {"OLLAMA_CONTEXT_LENGTH", ContextLength(), "Context length to use unless otherwise specified (default: 4096)"},
 		"OLLAMA_NEW_ENGINE":        {"OLLAMA_NEW_ENGINE", NewEngine(), "Enable the new Ollama engine"},
 
diff --git a/llama/llama.go b/llama/llama.go
index 0dc64e57..fe52b26d 100644
--- a/llama/llama.go
+++ b/llama/llama.go
@@ -9,6 +9,7 @@ package llama
 #cgo CPPFLAGS: -I${SRCDIR}/llama.cpp/tools/mtmd
 #cgo CPPFLAGS: -I${SRCDIR}/llama.cpp/src
 #cgo CPPFLAGS: -I${SRCDIR}/../ml/backend/ggml/ggml/include
+#cgo CPPFLAGS: -I${SRCDIR}/../ml/backend/ggml/
 
 #include <stdlib.h>
 #include "ggml.h"
@@ -18,6 +19,7 @@ package llama
 #include "gguf.h"
 
 #include "sampling_ext.h"
+#include "utils.h"
 
 extern bool llamaProgressCallback(float progress, void *user_data);
 extern void llamaLog(int level, char* text, void* user_data);
@@ -201,6 +203,7 @@ type ModelParams struct {
 	TensorSplit  []float32
 	Progress     func(float32)
 	VocabOnly    bool
+	RPCServers   string
 }
 
 //export llamaProgressCallback
@@ -212,6 +215,14 @@ func llamaProgressCallback(progress C.float, userData unsafe.Pointer) C.bool {
 }
 
 func LoadModelFromFile(modelPath string, params ModelParams) (*Model, error) {
+	if params.RPCServers != "" {
+		// Adding RPC servers to devices
+		rpcServers := C.CString(params.RPCServers)
+		C.add_rpc_devices(rpcServers)
+		C.free(unsafe.Pointer(rpcServers))
+	}
+
+	// Setting model parameters
 	cparams := C.llama_model_default_params()
 	cparams.n_gpu_layers = C.int(params.NumGpuLayers)
 	cparams.main_gpu = C.int32_t(params.MainGpu)
diff --git a/llama/patches/0001-ggml-backend-malloc-and-free-using-the-same-compiler.patched b/llama/patches/0001-ggml-backend-malloc-and-free-using-the-same-compiler.patched
new file mode 100644
index 00000000..e69de29b
diff --git a/llama/patches/0002-pretokenizer.patched b/llama/patches/0002-pretokenizer.patched
new file mode 100644
index 00000000..e69de29b
diff --git a/llama/patches/0003-embeddings.patched b/llama/patches/0003-embeddings.patched
new file mode 100644
index 00000000..e69de29b
diff --git a/llama/patches/0004-clip-unicode.patched b/llama/patches/0004-clip-unicode.patched
new file mode 100644
index 00000000..e69de29b
diff --git a/llama/patches/0005-solar-pro.patched b/llama/patches/0005-solar-pro.patched
new file mode 100644
index 00000000..e69de29b
diff --git a/llama/patches/0006-add-mllama-support.patched b/llama/patches/0006-add-mllama-support.patched
new file mode 100644
index 00000000..e69de29b
diff --git a/llama/patches/0007-add-unpad-operator.patched b/llama/patches/0007-add-unpad-operator.patched
new file mode 100644
index 00000000..e69de29b
diff --git a/llama/patches/0008-fix-deepseek-deseret-regex.patched b/llama/patches/0008-fix-deepseek-deseret-regex.patched
new file mode 100644
index 00000000..e69de29b
diff --git a/llama/patches/0009-maintain-ordering-for-rules-for-grammar.patched b/llama/patches/0009-maintain-ordering-for-rules-for-grammar.patched
new file mode 100644
index 00000000..e69de29b
diff --git a/llama/patches/0010-ensure-KV-cache-is-fully-defragmented.patched b/llama/patches/0010-ensure-KV-cache-is-fully-defragmented.patched
new file mode 100644
index 00000000..e69de29b
diff --git a/llama/patches/0011-sort-devices-by-score.patched b/llama/patches/0011-sort-devices-by-score.patched
new file mode 100644
index 00000000..e69de29b
diff --git a/llama/patches/0012-add-phony-target-ggml-cpu-for-all-cpu-variants.patched b/llama/patches/0012-add-phony-target-ggml-cpu-for-all-cpu-variants.patched
new file mode 100644
index 00000000..e69de29b
diff --git a/llama/patches/0013-remove-amx.patched b/llama/patches/0013-remove-amx.patched
new file mode 100644
index 00000000..e69de29b
diff --git a/llama/patches/0014-fix-string-arr-kv-loading.patched b/llama/patches/0014-fix-string-arr-kv-loading.patched
new file mode 100644
index 00000000..e69de29b
diff --git a/llama/patches/0015-ollama-debug-tensor.patched b/llama/patches/0015-ollama-debug-tensor.patched
new file mode 100644
index 00000000..e69de29b
diff --git a/llama/patches/0016-add-model-quantizations.patched b/llama/patches/0016-add-model-quantizations.patched
new file mode 100644
index 00000000..e69de29b
diff --git a/llama/patches/0017-add-ollama-vocab-for-grammar-support.patched b/llama/patches/0017-add-ollama-vocab-for-grammar-support.patched
new file mode 100644
index 00000000..e69de29b
diff --git a/llama/patches/0018-ggml-Don-t-assert-fail-when-tensor-data-changes-1322.patched b/llama/patches/0018-ggml-Don-t-assert-fail-when-tensor-data-changes-1322.patched
new file mode 100644
index 00000000..e69de29b
diff --git a/llm/server.go b/llm/server.go
index 7d921f14..1b7740ee 100644
--- a/llm/server.go
+++ b/llm/server.go
@@ -139,6 +139,18 @@ func NewLlamaServer(gpus discover.GpuInfoList, modelPath string, f *ggml.GGML, a
 		gpus = discover.GetCPUInfo()
 	}
 
+	rpcServers := ""
+	for _, gpu := range gpus {
+		if gpu.Library != "rpc" {
+			continue
+		}
+
+		if rpcServers != "" {
+			rpcServers += ","
+		}
+		rpcServers += gpu.ID
+	}
+
 	// Verify the requested context size is <= the model training size
 	trainCtx := f.KV().ContextLength()
 	if opts.NumCtx/numParallel > int(trainCtx) && trainCtx > 0 {
@@ -149,7 +161,7 @@ func NewLlamaServer(gpus discover.GpuInfoList, modelPath string, f *ggml.GGML, a
 	estimate := EstimateGPULayers(gpus, f, projectors, opts, numParallel)
 	if len(gpus) > 1 || gpus[0].Library != "cpu" {
 		switch {
-		case gpus[0].Library == "metal" && estimate.VRAMSize > systemTotalMemory:
+		case gpus[0].Library == "metal" && estimate.VRAMSize > systemTotalMemory && rpcServers == "":
 			// disable partial offloading when model is greater than total system memory as this
 			// can lead to locking up the system
 			opts.NumGPU = 0
@@ -184,6 +196,10 @@ func NewLlamaServer(gpus discover.GpuInfoList, modelPath string, f *ggml.GGML, a
 		params = append(params, "--n-gpu-layers", strconv.Itoa(opts.NumGPU))
 	}
 
+	if rpcServers != "" {
+		params = append(params, "--rpc", rpcServers)
+	}
+
 	if opts.MainGPU > 0 {
 		params = append(params, "--main-gpu", strconv.Itoa(opts.MainGPU))
 	}
@@ -230,8 +246,13 @@ func NewLlamaServer(gpus discover.GpuInfoList, modelPath string, f *ggml.GGML, a
 	}
 
 	// mmap has issues with partial offloading on metal
+	// rpc does not support mmap
 	for _, g := range gpus {
-		if g.Library == "metal" &&
+		if g.Library == "rpc" {
+			opts.UseMMap = new(bool)
+			*opts.UseMMap = false
+		}
+		if (g.Library == "metal") &&
 			uint64(opts.NumGPU) > 0 &&
 			uint64(opts.NumGPU) < f.KV().BlockCount()+1 {
 			opts.UseMMap = new(bool)
diff --git a/ml/backend.go b/ml/backend.go
index 06f9de9a..d46d054d 100644
--- a/ml/backend.go
+++ b/ml/backend.go
@@ -71,6 +71,9 @@ type BackendParams struct {
 
 	// FlashAttention indicates that we should use a fused flash attention kernel
 	FlashAttention bool
+
+	// RPCServers is a list of RPC servers available
+	RPCServers string
 }
 
 // ErrNoMem is returned when panicing due to insufficient memory. It includes
diff --git a/ml/backend/ggml/ggml.go b/ml/backend/ggml/ggml.go
index 24347689..11d45f98 100644
--- a/ml/backend/ggml/ggml.go
+++ b/ml/backend/ggml/ggml.go
@@ -6,6 +6,7 @@ package ggml
 // #include "ggml.h"
 // #include "ggml-cpu.h"
 // #include "ggml-backend.h"
+// #include "utils.h"
 import "C"
 
 import (
@@ -84,6 +85,13 @@ func New(modelPath string, params ml.BackendParams) (ml.Backend, error) {
 	}
 	defer r.Close()
 
+	// Add RPC servers to devices
+	if params.RPCServers != "" {
+		rpcServers := C.CString(params.RPCServers)
+		C.add_rpc_devices(rpcServers)
+		C.free(unsafe.Pointer(rpcServers))
+	}
+
 	meta, err := fsggml.Decode(r, -1)
 	if err != nil {
 		return nil, err
diff --git a/ml/backend/ggml/ggml/.rsync-filter b/ml/backend/ggml/ggml/.rsync-filter
index ddad16e2..a8d877cb 100644
--- a/ml/backend/ggml/ggml/.rsync-filter
+++ b/ml/backend/ggml/ggml/.rsync-filter
@@ -13,6 +13,7 @@ include src/ggml-cuda/vendors/
 include src/ggml-cuda/template-instances/
 include src/ggml-hip/
 include src/ggml-metal/
+include src/ggml-rpc/
 include *.c
 include *.h
 include *.cpp
diff --git a/ml/backend/ggml/ggml/include/rpc-server.h b/ml/backend/ggml/ggml/include/rpc-server.h
new file mode 100644
index 00000000..ff7a5632
--- /dev/null
+++ b/ml/backend/ggml/ggml/include/rpc-server.h
@@ -0,0 +1,10 @@
+
+#ifdef  __cplusplus
+extern "C" {
+#endif
+
+int run_rpc_server(const char *host, int port, const char *device);
+
+#ifdef  __cplusplus
+}
+#endif
\ No newline at end of file
diff --git a/ml/backend/ggml/ggml/src/ggml-rpc/CMakeLists.txt b/ml/backend/ggml/ggml/src/ggml-rpc/CMakeLists.txt
new file mode 100644
index 00000000..3200518b
--- /dev/null
+++ b/ml/backend/ggml/ggml/src/ggml-rpc/CMakeLists.txt
@@ -0,0 +1,10 @@
+message(STATUS "Using RPC backend")
+
+ggml_add_backend_library(ggml-rpc
+                         ggml-rpc.cpp
+                         rpc-server.cpp
+                        )
+
+if (WIN32)
+    target_link_libraries(ggml-rpc PRIVATE ws2_32)
+endif()
diff --git a/ml/backend/ggml/ggml/src/ggml-rpc/ggml-rpc.cpp b/ml/backend/ggml/ggml/src/ggml-rpc/ggml-rpc.cpp
new file mode 100644
index 00000000..d07e45d2
--- /dev/null
+++ b/ml/backend/ggml/ggml/src/ggml-rpc/ggml-rpc.cpp
@@ -0,0 +1,1824 @@
+#include "ggml-rpc.h"
+#include "ggml-impl.h"
+#include "ggml-backend-impl.h"
+#include "ggml-cpp.h"
+
+#include <cinttypes>
+#include <cstdio>
+#include <string>
+#include <vector>
+#include <memory>
+#include <mutex>
+#include <unordered_map>
+#include <unordered_set>
+#ifdef _WIN32
+#  define WIN32_LEAN_AND_MEAN
+#  ifndef NOMINMAX
+#     define NOMINMAX
+#  endif
+#  include <windows.h>
+#  include <winsock2.h>
+#else
+#  include <arpa/inet.h>
+#  include <sys/socket.h>
+#  include <sys/types.h>
+#  include <netinet/in.h>
+#  include <netinet/tcp.h>
+#  include <netdb.h>
+#  include <unistd.h>
+#endif
+#include <cstring>
+#include <fstream>
+#include <filesystem>
+#include <pthread.h>
+
+namespace fs = std::filesystem;
+
+#ifdef _WIN32
+typedef SOCKET sockfd_t;
+using ssize_t = __int64;
+#else
+typedef int sockfd_t;
+#endif
+
+// cross-platform socket
+struct socket_t {
+    sockfd_t fd;
+    socket_t(sockfd_t fd) : fd(fd) {}
+    ~socket_t() {
+        GGML_PRINT_DEBUG("[%s] closing socket %d\n", __func__, this->fd);
+#ifdef _WIN32
+        closesocket(this->fd);
+#else
+        close(this->fd);
+#endif
+    }
+    std::mutex send_rpc_cmd_mutex;
+};
+
+// all RPC structures must be packed
+#pragma pack(push, 1)
+// ggml_tensor is serialized into rpc_tensor
+struct rpc_tensor {
+    uint64_t id;
+    uint32_t type;
+    uint64_t buffer;
+    uint32_t ne[GGML_MAX_DIMS];
+    uint32_t nb[GGML_MAX_DIMS];
+    uint32_t op;
+    int32_t  op_params[GGML_MAX_OP_PARAMS / sizeof(int32_t)];
+    int32_t  flags;
+    uint64_t src[GGML_MAX_SRC];
+    uint64_t view_src;
+    uint64_t view_offs;
+    uint64_t data;
+    char name[GGML_MAX_NAME];
+
+    char padding[4];
+};
+
+static_assert(sizeof(rpc_tensor) % 8 == 0, "rpc_tensor size must be multiple of 8");
+
+// RPC commands
+enum rpc_cmd {
+    RPC_CMD_ALLOC_BUFFER = 0,
+    RPC_CMD_GET_ALIGNMENT,
+    RPC_CMD_GET_MAX_SIZE,
+    RPC_CMD_BUFFER_GET_BASE,
+    RPC_CMD_FREE_BUFFER,
+    RPC_CMD_BUFFER_CLEAR,
+    RPC_CMD_SET_TENSOR,
+    RPC_CMD_SET_TENSOR_HASH,
+    RPC_CMD_GET_TENSOR,
+    RPC_CMD_COPY_TENSOR,
+    RPC_CMD_GRAPH_COMPUTE,
+    RPC_CMD_GET_DEVICE_MEMORY,
+    RPC_CMD_INIT_TENSOR,
+    RPC_CMD_GET_ALLOC_SIZE,
+    RPC_CMD_HELLO,
+    RPC_CMD_COUNT,
+};
+
+const char* rpc_cmd_names[] = {
+    "RPC_CMD_ALLOC_BUFFER",
+    "RPC_CMD_GET_ALIGNMENT",
+    "RPC_CMD_GET_MAX_SIZE",
+    "RPC_CMD_BUFFER_GET_BASE",
+    "RPC_CMD_FREE_BUFFER",
+    "RPC_CMD_BUFFER_CLEAR",
+    "RPC_CMD_SET_TENSOR",
+    "RPC_CMD_SET_TENSOR_HASH",
+    "RPC_CMD_GET_TENSOR",
+    "RPC_CMD_COPY_TENSOR",
+    "RPC_CMD_GRAPH_COMPUTE",
+    "RPC_CMD_GET_DEVICE_MEMORY",
+    "RPC_CMD_INIT_TENSOR",
+    "RPC_CMD_GET_ALLOC_SIZE",
+    "RPC_CMD_HELLO"
+};
+
+// Try RPC_CMD_SET_TENSOR_HASH first when data size is larger than this threshold
+const size_t HASH_THRESHOLD = 10 * 1024 * 1024;
+
+struct rpc_msg_hello_rsp {
+    uint8_t major;
+    uint8_t minor;
+    uint8_t patch;
+};
+
+struct rpc_msg_get_alloc_size_req {
+    rpc_tensor tensor;
+};
+
+struct rpc_msg_get_alloc_size_rsp {
+    uint64_t alloc_size;
+};
+
+struct rpc_msg_init_tensor_req {
+    rpc_tensor tensor;
+};
+
+struct rpc_msg_alloc_buffer_req {
+    uint64_t size;
+};
+
+struct rpc_msg_alloc_buffer_rsp {
+    uint64_t remote_ptr;
+    uint64_t remote_size;
+};
+
+struct rpc_msg_get_alignment_rsp {
+    uint64_t alignment;
+};
+
+struct rpc_msg_get_max_size_rsp {
+    uint64_t max_size;
+};
+
+struct rpc_msg_buffer_get_base_req {
+    uint64_t remote_ptr;
+};
+
+struct rpc_msg_buffer_get_base_rsp {
+    uint64_t base_ptr;
+};
+
+struct rpc_msg_free_buffer_req {
+    uint64_t remote_ptr;
+};
+
+struct rpc_msg_buffer_clear_req {
+    uint64_t remote_ptr;
+    uint8_t value;
+};
+
+struct rpc_msg_set_tensor_hash_rsp {
+    uint8_t result;
+};
+
+struct rpc_msg_get_tensor_req {
+    rpc_tensor tensor;
+    uint64_t offset;
+    uint64_t size;
+};
+
+struct rpc_msg_copy_tensor_req {
+    rpc_tensor src;
+    rpc_tensor dst;
+};
+
+struct rpc_msg_copy_tensor_rsp {
+    uint8_t result;
+};
+
+struct rpc_msg_graph_compute_rsp {
+    uint8_t result;
+};
+
+struct rpc_msg_get_device_memory_rsp {
+    uint64_t free_mem;
+    uint64_t total_mem;
+};
+#pragma pack(pop)
+
+// RPC data structures
+
+static ggml_guid_t ggml_backend_rpc_guid() {
+    static ggml_guid guid = {0x99, 0x68, 0x5b, 0x6c, 0xd2, 0x83, 0x3d, 0x24, 0x25, 0x36, 0x72, 0xe1, 0x5b, 0x0e, 0x14, 0x03};
+    return &guid;
+}
+
+struct ggml_backend_rpc_buffer_type_context {
+    std::string endpoint;
+    std::string name;
+    size_t alignment;
+    size_t max_size;
+};
+
+struct ggml_backend_rpc_context {
+    std::string endpoint;
+    std::string name;
+};
+
+struct ggml_backend_rpc_buffer_context {
+    std::shared_ptr<socket_t> sock;
+    void * base_ptr;
+    uint64_t remote_ptr;
+};
+
+// RPC helper functions
+
+// Computes FNV-1a hash of the data
+static uint64_t fnv_hash(const uint8_t * data, size_t len) {
+    const uint64_t fnv_prime = 0x100000001b3ULL;
+    uint64_t hash = 0xcbf29ce484222325ULL;
+
+    for (size_t i = 0; i < len; ++i) {
+        hash ^= data[i];
+        hash *= fnv_prime;
+    }
+    return hash;
+}
+
+static std::shared_ptr<socket_t> make_socket(sockfd_t fd) {
+#ifdef _WIN32
+    if (fd == INVALID_SOCKET) {
+        return nullptr;
+    }
+#else
+    if (fd < 0) {
+        return nullptr;
+    }
+#endif
+    return std::make_shared<socket_t>(fd);
+}
+
+static bool set_no_delay(sockfd_t sockfd) {
+    int flag = 1;
+    // set TCP_NODELAY to disable Nagle's algorithm
+    int ret = setsockopt(sockfd, IPPROTO_TCP, TCP_NODELAY, (char *)&flag, sizeof(int));
+    return ret == 0;
+}
+
+static bool set_reuse_addr(sockfd_t sockfd) {
+    int flag = 1;
+    int ret = setsockopt(sockfd, SOL_SOCKET, SO_REUSEADDR, (char *)&flag, sizeof(int));
+    return ret == 0;
+}
+
+static std::shared_ptr<socket_t> socket_connect(const char * host, int port) {
+    struct sockaddr_in addr;
+    auto sockfd = socket(AF_INET, SOCK_STREAM, 0);
+    auto sock_ptr = make_socket(sockfd);
+    if (sock_ptr == nullptr) {
+        return nullptr;
+    }
+    if (!set_no_delay(sockfd)) {
+        fprintf(stderr, "Failed to set TCP_NODELAY\n");
+        return nullptr;
+    }
+    addr.sin_family = AF_INET;
+    addr.sin_port = htons(port);
+    struct hostent * server = gethostbyname(host);
+    if (server == NULL) {
+        fprintf(stderr, "Cannot resolve host '%s'\n", host);
+        return nullptr;
+    }
+    memcpy(&addr.sin_addr.s_addr, server->h_addr, server->h_length);
+    if (connect(sock_ptr->fd, (struct sockaddr *)&addr, sizeof(addr)) < 0) {
+        return nullptr;
+    }
+    return sock_ptr;
+}
+
+static std::shared_ptr<socket_t> socket_accept(sockfd_t srv_sockfd) {
+    auto client_socket_fd = accept(srv_sockfd, NULL, NULL);
+    auto client_socket = make_socket(client_socket_fd);
+    if (client_socket == nullptr) {
+        return nullptr;
+    }
+    if (!set_no_delay(client_socket_fd)) {
+        fprintf(stderr, "Failed to set TCP_NODELAY\n");
+        return nullptr;
+    }
+    return client_socket;
+}
+
+static std::shared_ptr<socket_t> create_server_socket(const char * host, int port) {
+    auto sockfd = socket(AF_INET, SOCK_STREAM, 0);
+    auto sock = make_socket(sockfd);
+    if (sock == nullptr) {
+        return nullptr;
+    }
+    if (!set_reuse_addr(sockfd)) {
+        fprintf(stderr, "Failed to set SO_REUSEADDR\n");
+        return nullptr;
+    }
+    if (inet_addr(host) == INADDR_NONE) {
+        fprintf(stderr, "Invalid host address: %s\n", host);
+        return nullptr;
+    }
+    struct sockaddr_in serv_addr;
+    serv_addr.sin_family = AF_INET;
+    serv_addr.sin_addr.s_addr = inet_addr(host);
+    serv_addr.sin_port = htons(port);
+
+    if (bind(sockfd, (struct sockaddr *) &serv_addr, sizeof(serv_addr)) < 0) {
+        return nullptr;
+    }
+    if (listen(sockfd, 1) < 0) {
+        return nullptr;
+    }
+    return sock;
+}
+
+static bool send_data(sockfd_t sockfd, const void * data, size_t size) {
+    size_t bytes_sent = 0;
+    while (bytes_sent < size) {
+        ssize_t n = send(sockfd, (const char *)data + bytes_sent, size - bytes_sent, 0);
+        if (n < 0) {
+            return false;
+        }
+        bytes_sent += n;
+    }
+    return true;
+}
+
+static bool recv_data(sockfd_t sockfd, void * data, size_t size) {
+    size_t bytes_recv = 0;
+    while (bytes_recv < size) {
+        ssize_t n = recv(sockfd, (char *)data + bytes_recv, size - bytes_recv, MSG_WAITALL);
+        if (n <= 0) {
+            return false;
+        }
+        bytes_recv += n;
+    }
+    return true;
+}
+
+static bool send_msg(sockfd_t sockfd, const void * msg, size_t msg_size) {
+    if (!send_data(sockfd, &msg_size, sizeof(msg_size))) {
+        return false;
+    }
+    return send_data(sockfd, msg, msg_size);
+}
+
+static bool recv_msg(sockfd_t sockfd, void * msg, size_t msg_size) {
+    uint64_t size;
+    if (!recv_data(sockfd, &size, sizeof(size))) {
+        return false;
+    }
+    if (size != msg_size) {
+        return false;
+    }
+    return recv_data(sockfd, msg, msg_size);
+}
+
+static bool recv_msg(sockfd_t sockfd, std::vector<uint8_t> & input) {
+    uint64_t size;
+    if (!recv_data(sockfd, &size, sizeof(size))) {
+        return false;
+    }
+    try {
+        input.resize(size);
+    } catch (const std::bad_alloc & e) {
+        fprintf(stderr, "Failed to allocate input buffer of size %" PRIu64 "\n", size);
+        return false;
+    }
+    return recv_data(sockfd, input.data(), size);
+}
+
+static bool parse_endpoint(const std::string & endpoint, std::string & host, int & port) {
+    size_t pos = endpoint.find(':');
+    if (pos == std::string::npos) {
+        return false;
+    }
+    host = endpoint.substr(0, pos);
+    port = std::stoi(endpoint.substr(pos + 1));
+    return true;
+}
+
+// RPC request : | rpc_cmd (1 byte) | request_size (8 bytes) | request_data (request_size bytes) |
+// No response
+static bool send_rpc_cmd(const std::shared_ptr<socket_t> & sock, enum rpc_cmd cmd, const void * input, size_t input_size) {
+    std::lock_guard<std::mutex> lock(sock->send_rpc_cmd_mutex);
+    uint8_t cmd_byte = cmd;
+    if (!send_data(sock->fd, &cmd_byte, sizeof(cmd_byte))) {
+        return false;
+    }
+    if (!send_data(sock->fd, &input_size, sizeof(input_size))) {
+        return false;
+    }
+    if (!send_data(sock->fd, input, input_size)) {
+        return false;
+    }
+    return true;
+}
+
+// RPC request : | rpc_cmd (1 byte) | request_size (8 bytes) | request_data (request_size bytes) |
+// RPC response: | response_size (8 bytes) | response_data (response_size bytes) |
+static bool send_rpc_cmd(const std::shared_ptr<socket_t> & sock, enum rpc_cmd cmd, const void * input, size_t input_size, void * output, size_t output_size) {
+    if (!send_rpc_cmd(sock, cmd, input, input_size)) {
+        return false;
+    }
+    // TODO: currently the output_size is always known, do we need support for commands with variable output size?
+    // even if we do, we can skip sending output_size from the server for commands with known output size
+    uint64_t out_size;
+    if (!recv_data(sock->fd, &out_size, sizeof(out_size))) {
+        return false;
+    }
+    if (out_size != output_size) {
+        return false;
+    }
+    if (!recv_data(sock->fd, output, output_size)) {
+        return false;
+    }
+    return true;
+}
+
+// RPC client-side implementation
+
+static bool check_server_version(const std::shared_ptr<socket_t> & sock) {
+    rpc_msg_hello_rsp response;
+    bool status = send_rpc_cmd(sock, RPC_CMD_HELLO, nullptr, 0, &response, sizeof(response));
+    GGML_ASSERT(status);
+    if (response.major != RPC_PROTO_MAJOR_VERSION || response.minor > RPC_PROTO_MINOR_VERSION) {
+        fprintf(stderr, "RPC server version mismatch: %d.%d.%d\n", response.major, response.minor, response.patch);
+        return false;
+    }
+    if (response.minor != RPC_PROTO_MINOR_VERSION || response.patch != RPC_PROTO_PATCH_VERSION) {
+        fprintf(stderr, "WARNING: RPC server version mismatch: %d.%d.%d\n", response.major, response.minor, response.patch);
+    }
+    return true;
+}
+
+static std::shared_ptr<socket_t> get_socket(const std::string & endpoint) {
+    static std::mutex mutex;
+    std::lock_guard<std::mutex> lock(mutex);
+    static std::unordered_map<std::string, std::weak_ptr<socket_t>> sockets;
+    static bool initialized = false;
+
+    auto it = sockets.find(endpoint);
+    if (it != sockets.end()) {
+        if (auto sock = it->second.lock()) {
+            return sock;
+        }
+    }
+    std::string host;
+    int port;
+    if (!parse_endpoint(endpoint, host, port)) {
+        return nullptr;
+    }
+#ifdef _WIN32
+    if (!initialized) {
+        WSADATA wsaData;
+        int res = WSAStartup(MAKEWORD(2, 2), &wsaData);
+        if (res != 0) {
+            return nullptr;
+        }
+        initialized = true;
+    }
+#else
+    GGML_UNUSED(initialized);
+#endif
+    auto sock = socket_connect(host.c_str(), port);
+    if (sock == nullptr) {
+        return nullptr;
+    }
+    if (!check_server_version(sock)) {
+        return nullptr;
+    }
+    GGML_PRINT_DEBUG("[%s] connected to %s, sockfd=%d\n", __func__, endpoint.c_str(), sock->fd);
+    sockets[endpoint] = sock;
+    return sock;
+}
+
+static void ggml_backend_rpc_buffer_free_buffer(ggml_backend_buffer_t buffer) {
+    ggml_backend_rpc_buffer_context * ctx = (ggml_backend_rpc_buffer_context *)buffer->context;
+    rpc_msg_free_buffer_req request = {ctx->remote_ptr};
+    bool status = send_rpc_cmd(ctx->sock, RPC_CMD_FREE_BUFFER, &request, sizeof(request), nullptr, 0);
+    GGML_ASSERT(status);
+    delete ctx;
+    delete buffer;
+}
+
+static void * ggml_backend_rpc_buffer_get_base(ggml_backend_buffer_t buffer) {
+    ggml_backend_rpc_buffer_context * ctx = (ggml_backend_rpc_buffer_context *)buffer->context;
+    if (ctx->base_ptr != nullptr) {
+        return ctx->base_ptr;
+    }
+    rpc_msg_buffer_get_base_req request = {ctx->remote_ptr};
+    rpc_msg_buffer_get_base_rsp response;
+    bool status = send_rpc_cmd(ctx->sock, RPC_CMD_BUFFER_GET_BASE, &request, sizeof(request), &response, sizeof(response));
+    GGML_ASSERT(status);
+    ctx->base_ptr = reinterpret_cast<void *>(response.base_ptr);
+    return ctx->base_ptr;
+}
+
+static rpc_tensor serialize_tensor(const ggml_tensor * tensor) {
+    rpc_tensor result;
+    result.id = reinterpret_cast<uint64_t>(tensor);
+    result.type = tensor->type;
+    if (tensor->buffer) {
+        ggml_backend_buffer_t buffer = tensor->buffer;
+        ggml_backend_rpc_buffer_context * ctx = (ggml_backend_rpc_buffer_context *)buffer->context;
+        result.buffer = ctx->remote_ptr;
+    } else {
+        result.buffer = 0;
+    }
+    for (uint32_t i = 0; i < GGML_MAX_DIMS; i++) {
+        result.ne[i] = tensor->ne[i];
+        result.nb[i] = tensor->nb[i];
+    }
+    result.op = tensor->op;
+    for (uint32_t i = 0; i < GGML_MAX_OP_PARAMS / sizeof(int32_t); i++) {
+        result.op_params[i] = tensor->op_params[i];
+    }
+    result.flags = tensor->flags;
+    for (uint32_t i = 0; i < GGML_MAX_SRC; i++) {
+        result.src[i] = reinterpret_cast<uint64_t>(tensor->src[i]);
+    }
+    result.view_src = reinterpret_cast<uint64_t>(tensor->view_src);
+    result.view_offs = tensor->view_offs;
+    result.data = reinterpret_cast<uint64_t>(tensor->data);
+    snprintf(result.name, GGML_MAX_NAME, "%s", tensor->name);
+    return result;
+}
+
+static enum ggml_status ggml_backend_rpc_buffer_init_tensor(ggml_backend_buffer_t buffer, ggml_tensor * tensor) {
+    ggml_backend_rpc_buffer_context * ctx = (ggml_backend_rpc_buffer_context *)buffer->context;
+
+    if (ggml_is_quantized(tensor->type) && (tensor->ne[0] % 512 != 0) && (tensor->view_src == nullptr)) {
+        rpc_msg_init_tensor_req request;
+
+        request.tensor = serialize_tensor(tensor);
+
+        bool status = send_rpc_cmd(ctx->sock, RPC_CMD_INIT_TENSOR, &request, sizeof(request), nullptr, 0);
+        GGML_ASSERT(status);
+    }
+    return GGML_STATUS_SUCCESS;
+}
+
+static void ggml_backend_rpc_buffer_set_tensor(ggml_backend_buffer_t buffer, ggml_tensor * tensor, const void * data, size_t offset, size_t size) {
+    ggml_backend_rpc_buffer_context * ctx = (ggml_backend_rpc_buffer_context *)buffer->context;
+    rpc_tensor rpc_tensor = serialize_tensor(tensor);
+    if (size > HASH_THRESHOLD) {
+        // input serialization format: | rpc_tensor | offset (8 bytes) | hash (8 bytes)
+        size_t input_size = sizeof(rpc_tensor) + sizeof(uint64_t) + sizeof(uint64_t);
+        std::vector<uint8_t> input(input_size, 0);
+        uint64_t hash = fnv_hash((const uint8_t*)data, size);
+        memcpy(input.data(), &rpc_tensor, sizeof(rpc_tensor));
+        memcpy(input.data() + sizeof(rpc_tensor), &offset, sizeof(offset));
+        memcpy(input.data() + sizeof(rpc_tensor) + sizeof(offset), &hash, sizeof(hash));
+        rpc_msg_set_tensor_hash_rsp response;
+        bool status = send_rpc_cmd(ctx->sock, RPC_CMD_SET_TENSOR_HASH, input.data(), input.size(), &response, sizeof(response));
+        GGML_ASSERT(status);
+        if (response.result) {
+            // the server has the same data, no need to send it
+            return;
+        }
+    }
+    // input serialization format: | rpc_tensor | offset (8 bytes) | data (size bytes)
+    size_t input_size = sizeof(rpc_tensor) + sizeof(uint64_t) + size;
+    std::vector<uint8_t> input(input_size, 0);
+    memcpy(input.data(), &rpc_tensor, sizeof(rpc_tensor));
+    memcpy(input.data() + sizeof(rpc_tensor), &offset, sizeof(offset));
+    memcpy(input.data() + sizeof(rpc_tensor) + sizeof(offset), data, size);
+    bool status = send_rpc_cmd(ctx->sock, RPC_CMD_SET_TENSOR, input.data(), input.size());
+    GGML_ASSERT(status);
+}
+
+static void ggml_backend_rpc_buffer_memset_tensor(ggml_backend_buffer_t buffer, ggml_tensor * tensor, uint8_t value, size_t offset, size_t size) {
+    ggml_backend_rpc_buffer_context * ctx = (ggml_backend_rpc_buffer_context *)buffer->context;
+    std::vector<uint8_t> data(size, value);
+    ggml_backend_rpc_buffer_set_tensor(buffer, tensor, data.data(), offset, size);
+}
+
+static void ggml_backend_rpc_buffer_get_tensor(ggml_backend_buffer_t buffer, const ggml_tensor * tensor, void * data, size_t offset, size_t size) {
+    ggml_backend_rpc_buffer_context * ctx = (ggml_backend_rpc_buffer_context *)buffer->context;
+    rpc_msg_get_tensor_req request;
+    request.tensor = serialize_tensor(tensor);
+    request.offset = offset;
+    request.size = size;
+    bool status = send_rpc_cmd(ctx->sock, RPC_CMD_GET_TENSOR, &request, sizeof(request), data, size);
+    GGML_ASSERT(status);
+}
+
+static bool ggml_backend_rpc_buffer_cpy_tensor(ggml_backend_buffer_t buffer, const ggml_tensor * src, ggml_tensor * dst) {
+    // check if src and dst are on the same server
+    ggml_backend_buffer_t src_buffer = src->buffer;
+    ggml_backend_rpc_buffer_context * src_ctx = (ggml_backend_rpc_buffer_context *)src_buffer->context;
+    ggml_backend_buffer_t dst_buffer = dst->buffer;
+    ggml_backend_rpc_buffer_context * dst_ctx = (ggml_backend_rpc_buffer_context *)dst_buffer->context;
+    if (src_ctx->sock != dst_ctx->sock) {
+        return false;
+    }
+    ggml_backend_rpc_buffer_context * ctx = (ggml_backend_rpc_buffer_context *)buffer->context;
+    rpc_msg_copy_tensor_req request;
+    request.src = serialize_tensor(src);
+    request.dst = serialize_tensor(dst);
+    rpc_msg_copy_tensor_rsp response;
+    bool status = send_rpc_cmd(ctx->sock, RPC_CMD_COPY_TENSOR, &request, sizeof(request), &response, sizeof(response));
+    GGML_ASSERT(status);
+    return response.result;
+}
+
+static void ggml_backend_rpc_buffer_clear(ggml_backend_buffer_t buffer, uint8_t value) {
+    ggml_backend_rpc_buffer_context * ctx = (ggml_backend_rpc_buffer_context *)buffer->context;
+    rpc_msg_buffer_clear_req request = {ctx->remote_ptr, value};
+    bool status = send_rpc_cmd(ctx->sock, RPC_CMD_BUFFER_CLEAR, &request, sizeof(request), nullptr, 0);
+    GGML_ASSERT(status);
+}
+
+static ggml_backend_buffer_i ggml_backend_rpc_buffer_interface = {
+    /* .free_buffer     = */ ggml_backend_rpc_buffer_free_buffer,
+    /* .get_base        = */ ggml_backend_rpc_buffer_get_base,
+    /* .init_tensor     = */ ggml_backend_rpc_buffer_init_tensor,
+    /* .memset_tensor   = */ ggml_backend_rpc_buffer_memset_tensor,
+    /* .set_tensor      = */ ggml_backend_rpc_buffer_set_tensor,
+    /* .get_tensor      = */ ggml_backend_rpc_buffer_get_tensor,
+    /* .cpy_tensor      = */ ggml_backend_rpc_buffer_cpy_tensor,
+    /* .clear           = */ ggml_backend_rpc_buffer_clear,
+    /* .reset           = */ NULL,
+};
+
+static const char * ggml_backend_rpc_buffer_type_name(ggml_backend_buffer_type_t buft) {
+    ggml_backend_rpc_buffer_type_context * buft_ctx = (ggml_backend_rpc_buffer_type_context *)buft->context;
+    return buft_ctx->name.c_str();
+}
+
+static ggml_backend_buffer_t ggml_backend_rpc_buffer_type_alloc_buffer(ggml_backend_buffer_type_t buft, size_t size) {
+    ggml_backend_rpc_buffer_type_context * buft_ctx = (ggml_backend_rpc_buffer_type_context *)buft->context;
+    rpc_msg_alloc_buffer_req request = {size};
+    rpc_msg_alloc_buffer_rsp response;
+    auto sock = get_socket(buft_ctx->endpoint);
+    bool status = send_rpc_cmd(sock, RPC_CMD_ALLOC_BUFFER, &request, sizeof(request), &response, sizeof(response));
+    GGML_ASSERT(status);
+    if (response.remote_ptr != 0) {
+        ggml_backend_buffer_t buffer = ggml_backend_buffer_init(buft,
+            ggml_backend_rpc_buffer_interface,
+            new ggml_backend_rpc_buffer_context{sock, nullptr, response.remote_ptr},
+            response.remote_size);
+        return buffer;
+    } else {
+        return nullptr;
+    }
+}
+
+static size_t get_alignment(const std::shared_ptr<socket_t> & sock) {
+    rpc_msg_get_alignment_rsp response;
+    bool status = send_rpc_cmd(sock, RPC_CMD_GET_ALIGNMENT, nullptr, 0, &response, sizeof(response));
+    GGML_ASSERT(status);
+    return response.alignment;
+}
+
+static size_t ggml_backend_rpc_buffer_type_get_alignment(ggml_backend_buffer_type_t buft) {
+    ggml_backend_rpc_buffer_type_context * buft_ctx = (ggml_backend_rpc_buffer_type_context *)buft->context;
+    return buft_ctx->alignment;
+}
+
+static size_t get_max_size(const std::shared_ptr<socket_t> & sock) {
+    rpc_msg_get_max_size_rsp response;
+    bool status = send_rpc_cmd(sock, RPC_CMD_GET_MAX_SIZE, nullptr, 0, &response, sizeof(response));
+    GGML_ASSERT(status);
+    return response.max_size;
+}
+
+static size_t ggml_backend_rpc_get_max_size(ggml_backend_buffer_type_t buft) {
+    ggml_backend_rpc_buffer_type_context * buft_ctx = (ggml_backend_rpc_buffer_type_context *)buft->context;
+    return buft_ctx->max_size;
+}
+
+static size_t ggml_backend_rpc_buffer_type_get_alloc_size(ggml_backend_buffer_type_t buft, const ggml_tensor * tensor) {
+    // See comments in init_tensor.
+    if (ggml_is_quantized(tensor->type) && (tensor->ne[0] % 512 != 0) && (tensor->view_src == nullptr)) {
+        ggml_backend_rpc_buffer_type_context * buft_ctx = (ggml_backend_rpc_buffer_type_context *)buft->context;
+        auto sock = get_socket(buft_ctx->endpoint);
+
+        rpc_msg_get_alloc_size_req request;
+
+        request.tensor = serialize_tensor(tensor);
+
+        rpc_msg_get_alloc_size_rsp response;
+        bool status = send_rpc_cmd(sock, RPC_CMD_GET_ALLOC_SIZE, &request, sizeof(request), &response, sizeof(response));
+        GGML_ASSERT(status);
+
+        return response.alloc_size;
+    } else {
+        return ggml_nbytes(tensor);
+    }
+}
+
+static ggml_backend_buffer_type_i ggml_backend_rpc_buffer_type_interface = {
+    /* .get_name         = */ ggml_backend_rpc_buffer_type_name,
+    /* .alloc_buffer     = */ ggml_backend_rpc_buffer_type_alloc_buffer,
+    /* .get_alignment    = */ ggml_backend_rpc_buffer_type_get_alignment,
+    /* .get_max_size     = */ ggml_backend_rpc_get_max_size,
+    /* .get_alloc_size   = */ ggml_backend_rpc_buffer_type_get_alloc_size,
+    /* .is_host          = */ NULL,
+};
+
+static const char * ggml_backend_rpc_name(ggml_backend_t backend) {
+    ggml_backend_rpc_context * rpc_ctx = (ggml_backend_rpc_context *)backend->context;
+
+    return rpc_ctx->name.c_str();
+}
+
+static void ggml_backend_rpc_free(ggml_backend_t backend) {
+    ggml_backend_rpc_context * rpc_ctx = (ggml_backend_rpc_context *)backend->context;
+    delete rpc_ctx;
+    delete backend;
+}
+
+static void ggml_backend_rpc_synchronize(ggml_backend_t backend) {
+    GGML_UNUSED(backend);
+    // this is no-op because we don't have any async operations
+}
+
+static void add_tensor(ggml_tensor * tensor, std::vector<rpc_tensor> & tensors, std::unordered_set<ggml_tensor*> & visited) {
+    if (tensor == nullptr) {
+        return;
+    }
+    if (visited.find(tensor) != visited.end()) {
+        return;
+    }
+    visited.insert(tensor);
+    for (int i = 0; i < GGML_MAX_SRC; i++) {
+        add_tensor(tensor->src[i], tensors, visited);
+    }
+    add_tensor(tensor->view_src, tensors, visited);
+    tensors.push_back(serialize_tensor(tensor));
+}
+
+static void serialize_graph(const ggml_cgraph * cgraph, std::vector<uint8_t> & output) {
+    uint32_t n_nodes = cgraph->n_nodes;
+    std::vector<rpc_tensor> tensors;
+    std::unordered_set<ggml_tensor*> visited;
+    for (uint32_t i = 0; i < n_nodes; i++) {
+        add_tensor(cgraph->nodes[i], tensors, visited);
+    }
+    // serialization format:
+    // | n_nodes (4 bytes) | nodes (n_nodes * sizeof(uint64_t) | n_tensors (4 bytes) | tensors (n_tensors * sizeof(rpc_tensor)) |
+    uint32_t n_tensors = tensors.size();
+    int output_size = sizeof(uint32_t) + n_nodes * sizeof(uint64_t) + sizeof(uint32_t) + n_tensors * sizeof(rpc_tensor);
+    output.resize(output_size, 0);
+    memcpy(output.data(), &n_nodes, sizeof(n_nodes));
+    for (uint32_t i = 0; i < n_nodes; i++) {
+        memcpy(output.data() + sizeof(n_nodes) + i * sizeof(uint64_t), &cgraph->nodes[i], sizeof(uint64_t));
+    }
+    uint32_t * out_ntensors = (uint32_t *)(output.data() + sizeof(n_nodes) + n_nodes * sizeof(uint64_t));
+    *out_ntensors = n_tensors;
+    rpc_tensor * out_tensors = (rpc_tensor *)(output.data() + sizeof(n_nodes) + n_nodes * sizeof(uint64_t) + sizeof(uint32_t));
+    memcpy(out_tensors, tensors.data(), n_tensors * sizeof(rpc_tensor));
+}
+
+static enum ggml_status ggml_backend_rpc_graph_compute(ggml_backend_t backend, ggml_cgraph * cgraph) {
+    ggml_backend_rpc_context * rpc_ctx = (ggml_backend_rpc_context *)backend->context;
+    std::vector<uint8_t> input;
+    serialize_graph(cgraph, input);
+    rpc_msg_graph_compute_rsp response;
+    auto sock = get_socket(rpc_ctx->endpoint);
+    bool status = send_rpc_cmd(sock, RPC_CMD_GRAPH_COMPUTE, input.data(), input.size(), &response, sizeof(response));
+    GGML_ASSERT(status);
+    return (enum ggml_status)response.result;
+}
+
+static ggml_backend_i ggml_backend_rpc_interface = {
+    /* .get_name                = */ ggml_backend_rpc_name,
+    /* .free                    = */ ggml_backend_rpc_free,
+    /* .set_tensor_async        = */ NULL,
+    /* .get_tensor_async        = */ NULL,
+    /* .cpy_tensor_async        = */ NULL,
+    /* .synchronize             = */ ggml_backend_rpc_synchronize,
+    /* .graph_plan_create       = */ NULL,
+    /* .graph_plan_free         = */ NULL,
+    /* .graph_plan_update       = */ NULL,
+    /* .graph_plan_compute      = */ NULL,
+    /* .graph_compute           = */ ggml_backend_rpc_graph_compute,
+    /* .event_record            = */ NULL,
+    /* .event_wait              = */ NULL,
+};
+
+ggml_backend_buffer_type_t ggml_backend_rpc_buffer_type(const char * endpoint) {
+    static std::mutex mutex;
+    std::lock_guard<std::mutex> lock(mutex);
+    // NOTE: buffer types are allocated and never freed; this is by design
+    static std::unordered_map<std::string, ggml_backend_buffer_type_t> buft_map;
+    auto it = buft_map.find(endpoint);
+    if (it != buft_map.end()) {
+        return it->second;
+    }
+    auto sock = get_socket(endpoint);
+    if (sock == nullptr) {
+        fprintf(stderr, "Failed to connect to %s\n", endpoint);
+        return nullptr;
+    }
+    size_t alignment = get_alignment(sock);
+    size_t max_size = get_max_size(sock);
+    ggml_backend_rpc_buffer_type_context * buft_ctx = new ggml_backend_rpc_buffer_type_context {
+        /* .endpoint  = */ endpoint,
+        /* .name      = */ "RPC[" + std::string(endpoint) + "]",
+        /* .alignment = */ alignment,
+        /* .max_size  = */ max_size
+    };
+
+    ggml_backend_buffer_type_t buft = new ggml_backend_buffer_type {
+        /* .iface   = */ ggml_backend_rpc_buffer_type_interface,
+        /* .device  = */ ggml_backend_rpc_add_device(endpoint),
+        /* .context = */ buft_ctx
+    };
+    buft_map[endpoint] = buft;
+    return buft;
+}
+
+ggml_backend_t ggml_backend_rpc_init(const char * endpoint) {
+    ggml_backend_rpc_context * ctx = new ggml_backend_rpc_context {
+        /* .endpoint  = */ endpoint,
+        /* .name      = */ "RPC[" + std::string(endpoint) + "]",
+    };
+
+    ggml_backend_t backend = new ggml_backend {
+        /* .guid      = */ ggml_backend_rpc_guid(),
+        /* .interface = */ ggml_backend_rpc_interface,
+        /* .device    = */ ggml_backend_rpc_add_device(endpoint),
+        /* .context   = */ ctx
+    };
+    return backend;
+}
+
+bool ggml_backend_is_rpc(ggml_backend_t backend) {
+    return backend != NULL && ggml_guid_matches(backend->guid, ggml_backend_rpc_guid());
+}
+
+static void get_device_memory(const std::shared_ptr<socket_t> & sock, size_t * free, size_t * total) {
+    rpc_msg_get_device_memory_rsp response;
+    bool status = send_rpc_cmd(sock, RPC_CMD_GET_DEVICE_MEMORY, nullptr, 0, &response, sizeof(response));
+    GGML_ASSERT(status);
+    *free = response.free_mem;
+    *total = response.total_mem;
+}
+
+void ggml_backend_rpc_get_device_memory(const char * endpoint, size_t * free, size_t * total) {
+    auto sock = get_socket(endpoint);
+    if (sock == nullptr) {
+        *free = 0;
+        *total = 0;
+        return;
+    }
+    get_device_memory(sock, free, total);
+}
+
+// RPC server-side implementation
+
+class rpc_server {
+public:
+    rpc_server(ggml_backend_t backend, const char * cache_dir)
+        : backend(backend), cache_dir(cache_dir) {
+    }
+    ~rpc_server();
+
+    void hello(rpc_msg_hello_rsp & response);
+    void alloc_buffer(const rpc_msg_alloc_buffer_req & request, rpc_msg_alloc_buffer_rsp & response);
+    void get_alignment(rpc_msg_get_alignment_rsp & response);
+    void get_max_size(rpc_msg_get_max_size_rsp & response);
+    bool buffer_get_base(const rpc_msg_buffer_get_base_req & request, rpc_msg_buffer_get_base_rsp & response);
+    bool free_buffer(const rpc_msg_free_buffer_req & request);
+    bool buffer_clear(const rpc_msg_buffer_clear_req & request);
+    bool set_tensor(const std::vector<uint8_t> & input);
+    bool set_tensor_hash(const std::vector<uint8_t> & input, rpc_msg_set_tensor_hash_rsp & response);
+    bool get_tensor(const rpc_msg_get_tensor_req & request, std::vector<uint8_t> & response);
+    bool copy_tensor(const rpc_msg_copy_tensor_req & request, rpc_msg_copy_tensor_rsp & response);
+    bool graph_compute(const std::vector<uint8_t> & input, rpc_msg_graph_compute_rsp & response);
+    bool init_tensor(const rpc_msg_init_tensor_req & request);
+    bool get_alloc_size(const rpc_msg_get_alloc_size_req & request, rpc_msg_get_alloc_size_rsp & response);
+
+private:
+    bool get_cached_file(uint64_t hash, std::vector<uint8_t> & data);
+    ggml_tensor * deserialize_tensor(struct ggml_context * ctx, const rpc_tensor * tensor);
+    ggml_tensor * create_node(uint64_t id,
+                              struct ggml_context * ctx,
+                              const std::unordered_map<uint64_t, const rpc_tensor*> & tensor_ptrs,
+                              std::unordered_map<uint64_t, struct ggml_tensor*> & tensor_map);
+
+
+    ggml_backend_t backend;
+    const char * cache_dir;
+    std::unordered_set<ggml_backend_buffer_t> buffers;
+};
+
+void rpc_server::hello(rpc_msg_hello_rsp & response) {
+    response.major = RPC_PROTO_MAJOR_VERSION;
+    response.minor = RPC_PROTO_MINOR_VERSION;
+    response.patch = RPC_PROTO_PATCH_VERSION;
+    GGML_PRINT_DEBUG("[%s] version: %d.%d.%d\n", __func__, response.major, response.minor, response.patch);
+}
+
+bool rpc_server::get_alloc_size(const rpc_msg_get_alloc_size_req & request, rpc_msg_get_alloc_size_rsp & response) {
+    ggml_backend_buffer_type_t buft;
+    struct ggml_init_params params {
+        /*.mem_size   =*/ ggml_tensor_overhead(),
+        /*.mem_buffer =*/ NULL,
+        /*.no_alloc   =*/ true,
+    };
+
+    ggml_context_ptr ctx_ptr { ggml_init(params) };
+    GGML_ASSERT(ctx_ptr != nullptr);
+    ggml_context * ctx = ctx_ptr.get();
+    ggml_tensor * tensor = deserialize_tensor(ctx, &request.tensor);
+
+    if (tensor == nullptr) {
+        GGML_LOG_ERROR("Null tensor pointer passed to server get_alloc_size function.\n");
+        return false;
+    }
+
+    if (tensor->buffer == nullptr) {
+        //No buffer allocated.
+        buft = ggml_backend_get_default_buffer_type(backend);
+    } else {
+        buft = tensor->buffer->buft;
+    }
+
+    response.alloc_size = ggml_backend_buft_get_alloc_size(buft,tensor);
+
+    return true;
+}
+
+void rpc_server::alloc_buffer(const rpc_msg_alloc_buffer_req & request, rpc_msg_alloc_buffer_rsp & response) {
+    ggml_backend_buffer_type_t buft = ggml_backend_get_default_buffer_type(backend);
+    ggml_backend_buffer_t buffer = ggml_backend_buft_alloc_buffer(buft, request.size);
+    response.remote_ptr = 0;
+    response.remote_size = 0;
+    if (buffer != nullptr) {
+        response.remote_ptr = reinterpret_cast<uint64_t>(buffer);
+        response.remote_size = buffer->size;
+        buffers.insert(buffer);
+    } else {
+        GGML_LOG_ERROR("[%s] size: %" PRIu64 " -> failed\n", __func__, request.size);
+    }
+}
+
+void rpc_server::get_alignment(rpc_msg_get_alignment_rsp & response) {
+    ggml_backend_buffer_type_t buft = ggml_backend_get_default_buffer_type(backend);
+    size_t alignment = ggml_backend_buft_get_alignment(buft);
+    GGML_PRINT_DEBUG("[%s] alignment: %lu\n", __func__, alignment);
+    response.alignment = alignment;
+}
+
+void rpc_server::get_max_size(rpc_msg_get_max_size_rsp & response) {
+    ggml_backend_buffer_type_t buft = ggml_backend_get_default_buffer_type(backend);
+    size_t max_size = ggml_backend_buft_get_max_size(buft);
+    GGML_PRINT_DEBUG("[%s] max_size: %lu\n", __func__, max_size);
+    response.max_size = max_size;
+}
+
+bool rpc_server::buffer_get_base(const rpc_msg_buffer_get_base_req & request, rpc_msg_buffer_get_base_rsp & response) {
+    GGML_PRINT_DEBUG("[%s] remote_ptr: %" PRIx64 "\n", __func__, request.remote_ptr);
+    ggml_backend_buffer_t buffer = reinterpret_cast<ggml_backend_buffer_t>(request.remote_ptr);
+    if (buffers.find(buffer) == buffers.end()) {
+        GGML_LOG_ERROR("[%s] buffer not found\n", __func__);
+        return false;
+    }
+    void * base = ggml_backend_buffer_get_base(buffer);
+    response.base_ptr = reinterpret_cast<uint64_t>(base);
+    return true;
+}
+
+bool rpc_server::free_buffer(const rpc_msg_free_buffer_req & request) {
+    GGML_PRINT_DEBUG("[%s] remote_ptr: %" PRIx64 "\n", __func__, request.remote_ptr);
+    ggml_backend_buffer_t buffer = reinterpret_cast<ggml_backend_buffer_t>(request.remote_ptr);
+    if (buffers.find(buffer) == buffers.end()) {
+        GGML_LOG_ERROR("[%s] buffer not found\n", __func__);
+        return false;
+    }
+    ggml_backend_buffer_free(buffer);
+    buffers.erase(buffer);
+    return true;
+}
+
+bool rpc_server::buffer_clear(const rpc_msg_buffer_clear_req & request) {
+    GGML_PRINT_DEBUG("[%s] remote_ptr: %" PRIx64 ", value: %u\n", __func__, request.remote_ptr, request.value);
+    ggml_backend_buffer_t buffer = reinterpret_cast<ggml_backend_buffer_t>(request.remote_ptr);
+    if (buffers.find(buffer) == buffers.end()) {
+        GGML_LOG_ERROR("[%s] buffer not found\n", __func__);
+        return false;
+    }
+    ggml_backend_buffer_clear(buffer, request.value);
+    return true;
+}
+
+ggml_tensor * rpc_server::deserialize_tensor(struct ggml_context * ctx, const rpc_tensor * tensor) {
+    // Validate tensor type before using it
+    if (tensor->type >= GGML_TYPE_COUNT) {
+        GGML_LOG_ERROR("[%s] invalid tensor type received: %u\n", __func__, tensor->type);
+        return nullptr;
+    }
+
+    ggml_tensor * result = ggml_new_tensor_4d(ctx, (ggml_type) tensor->type,
+        tensor->ne[0], tensor->ne[1], tensor->ne[2], tensor->ne[3]);
+
+    // ggml_new_tensor_4d might fail if dimensions are invalid, although less likely to crash than invalid type
+    if (result == nullptr) {
+        GGML_LOG_ERROR("[%s] ggml_new_tensor_4d failed for type %u\\n", __func__, tensor->type);
+        return nullptr;
+    }
+
+    for (uint32_t i = 0; i < GGML_MAX_DIMS; i++) {
+        result->nb[i] = tensor->nb[i];
+    }
+    result->buffer = reinterpret_cast<ggml_backend_buffer_t>(tensor->buffer);
+    if (result->buffer && buffers.find(result->buffer) == buffers.end()) {
+        result->buffer = nullptr;
+    }
+
+    if (result->buffer) {
+        // require that the tensor data does not go beyond the buffer end
+        uint64_t tensor_size = (uint64_t) ggml_nbytes(result);
+        uint64_t buffer_start = (uint64_t) ggml_backend_buffer_get_base(result->buffer);
+        uint64_t buffer_size = (uint64_t) ggml_backend_buffer_get_size(result->buffer);
+        GGML_ASSERT(tensor->data + tensor_size >= tensor->data); // check for overflow
+        GGML_ASSERT(tensor->data >= buffer_start && tensor->data + tensor_size <= buffer_start + buffer_size);
+    }
+
+    result->op = (ggml_op) tensor->op;
+    for (uint32_t i = 0; i < GGML_MAX_OP_PARAMS / sizeof(int32_t); i++) {
+        result->op_params[i] = tensor->op_params[i];
+    }
+    result->flags = tensor->flags;
+    result->data = reinterpret_cast<void *>(tensor->data);
+    ggml_set_name(result, tensor->name);
+    return result;
+}
+
+
+bool rpc_server::set_tensor(const std::vector<uint8_t> & input) {
+    // serialization format: | rpc_tensor | offset (8 bytes) | data (size bytes) |
+    if (input.size() < sizeof(rpc_tensor) + sizeof(uint64_t)) {
+        return false;
+    }
+    const rpc_tensor * in_tensor = (const rpc_tensor *)input.data();
+    uint64_t offset;
+    memcpy(&offset, input.data() + sizeof(rpc_tensor), sizeof(offset));
+    const size_t size = input.size() - sizeof(rpc_tensor) - sizeof(offset);
+
+    struct ggml_init_params params {
+        /*.mem_size   =*/ ggml_tensor_overhead(),
+        /*.mem_buffer =*/ NULL,
+        /*.no_alloc   =*/ true,
+    };
+    ggml_context_ptr ctx_ptr { ggml_init(params) };
+    GGML_ASSERT(ctx_ptr != nullptr);
+    ggml_context * ctx = ctx_ptr.get();
+    ggml_tensor * tensor = deserialize_tensor(ctx, in_tensor);
+    if (tensor == nullptr) {
+        GGML_LOG_ERROR("[%s] error deserializing tensor\n", __func__);
+        return false;
+    }
+    GGML_PRINT_DEBUG("[%s] buffer: %p, data: %p, offset: %" PRIu64 ", size: %zu\n", __func__, (void*)tensor->buffer, tensor->data, offset, size);
+
+    // sanitize tensor->data
+    {
+        const size_t p0 = (size_t) ggml_backend_buffer_get_base(tensor->buffer);
+        const size_t p1 = p0 + ggml_backend_buffer_get_size(tensor->buffer);
+
+        if (in_tensor->data + offset < p0 || in_tensor->data + offset >= p1 || size > (p1 - in_tensor->data - offset)) {
+            GGML_LOG_ERROR("[%s] tensor data region (data=0x%" PRIx64 ", offset=%" PRIu64 ", size=%zu) out of buffer bounds [0x%zx, 0x%zx)\n",
+                           __func__, in_tensor->data, offset, size, p0, p1);
+            return false;
+        }
+    }
+
+    const void * data = input.data() + sizeof(rpc_tensor) + sizeof(offset);
+    if (cache_dir && size > HASH_THRESHOLD) {
+        uint64_t hash = fnv_hash((const uint8_t*)data, size);
+        char hash_str[17];
+        snprintf(hash_str, sizeof(hash_str), "%016" PRIx64, hash);
+        // save to cache_dir/hash_str
+        fs::path cache_file = fs::path(cache_dir) / hash_str;
+        std::ofstream ofs(cache_file, std::ios::binary);
+        ofs.write((const char *)data, size);
+        printf("[%s] saved to '%s'\n", __func__, cache_file.c_str());
+    }
+    ggml_backend_tensor_set(tensor, data, offset, size);
+    return true;
+}
+
+bool rpc_server::get_cached_file(uint64_t hash, std::vector<uint8_t> & data) {
+    if (!cache_dir) {
+        return false;
+    }
+    char hash_str[17];
+    snprintf(hash_str, sizeof(hash_str), "%016" PRIx64, hash);
+    fs::path cache_file = fs::path(cache_dir) / hash_str;
+    if (!fs::exists(cache_file)) {
+        return false;
+    }
+    std::ifstream ifs(cache_file, std::ios::binary);
+    ifs.seekg(0, std::ios::end);
+    size_t size = ifs.tellg();
+    ifs.seekg(0, std::ios::beg);
+    data.resize(size);
+    ifs.read((char *)data.data(), size);
+    return true;
+}
+
+bool rpc_server::set_tensor_hash(const std::vector<uint8_t> & input, rpc_msg_set_tensor_hash_rsp & response)
+{
+    // serialization format: | rpc_tensor | offset (8 bytes) | hash (8 bytes) |
+    if (input.size() != sizeof(rpc_tensor) + 16) {
+        return false;
+    }
+    const rpc_tensor * in_tensor = (const rpc_tensor *)input.data();
+    uint64_t offset;
+    memcpy(&offset, input.data() + sizeof(rpc_tensor), sizeof(offset));
+    const uint64_t * hash = (const uint64_t *)(input.data() + sizeof(rpc_tensor) + sizeof(offset));
+    std::vector<uint8_t> cached_file;
+    if (!get_cached_file(*hash, cached_file)) {
+        response.result = 0;
+        return true;
+    }
+    size_t size = cached_file.size();
+    struct ggml_init_params params {
+        /*.mem_size   =*/ ggml_tensor_overhead(),
+        /*.mem_buffer =*/ NULL,
+        /*.no_alloc   =*/ true,
+    };
+    ggml_context_ptr ctx_ptr { ggml_init(params) };
+    GGML_ASSERT(ctx_ptr != nullptr);
+    ggml_context * ctx = ctx_ptr.get();
+    ggml_tensor * tensor = deserialize_tensor(ctx, in_tensor);
+    if (tensor == nullptr) {
+        GGML_LOG_ERROR("[%s] error deserializing tensor\n", __func__);
+        return false;
+    }
+    GGML_PRINT_DEBUG("[%s] buffer: %p, data: %p, offset: %" PRIu64 ", size: %zu, hash: %" PRIx64 "\n", __func__, (void*)tensor->buffer, tensor->data, offset, size, *hash);
+
+    // sanitize tensor->data
+    {
+        const size_t p0 = (size_t) ggml_backend_buffer_get_base(tensor->buffer);
+        const size_t p1 = p0 + ggml_backend_buffer_get_size(tensor->buffer);
+
+        if (in_tensor->data + offset < p0 || in_tensor->data + offset >= p1 || size > (p1 - in_tensor->data - offset)) {
+            GGML_LOG_ERROR("[%s] tensor data region (data=0x%" PRIx64 ", offset=%" PRIu64 ", size=%zu, hash=0x%" PRIx64 ") out of buffer bounds [0x%zx, 0x%zx)\n",
+                           __func__, in_tensor->data, offset, size, *hash, p0, p1);
+            return false;
+        }
+    }
+    ggml_backend_tensor_set(tensor, cached_file.data(), offset, size);
+    response.result = 1;
+    return true;
+}
+
+bool rpc_server::init_tensor(const rpc_msg_init_tensor_req & request) {
+    struct ggml_init_params params {
+        /*.mem_size   =*/ ggml_tensor_overhead(),
+        /*.mem_buffer =*/ NULL,
+        /*.no_alloc   =*/ true,
+    };
+    ggml_context_ptr ctx_ptr { ggml_init(params) };
+    GGML_ASSERT(ctx_ptr != nullptr);
+    ggml_context * ctx = ctx_ptr.get();
+    ggml_tensor * tensor = deserialize_tensor(ctx, &request.tensor);
+    if (tensor == nullptr) {
+        GGML_LOG_ERROR("Null tensor pointer passed to server init_tensor function.\n");
+        return false;
+    }
+
+    // Call the backend's buffer_init_tensor function
+    ggml_backend_buffer_t buffer = tensor->buffer;
+    if (buffer && buffer->iface.init_tensor) {
+        buffer->iface.init_tensor(buffer, tensor);
+    } else {
+        GGML_LOG_DEBUG("Null buffer for tensor passed to init_tensor function\n");
+    }
+
+    if (tensor->extra != nullptr) {
+        // This pointer can either be passed around client/server, or probably better stored server-side and kept track of.
+        // Currently unimplemented.
+        GGML_LOG_ERROR("tensor->extra populated by the backend, this is currently unsupported.\n");
+        return false;
+    }
+
+    return true;
+}
+
+bool rpc_server::get_tensor(const rpc_msg_get_tensor_req & request, std::vector<uint8_t> & response) {
+    struct ggml_init_params params {
+        /*.mem_size   =*/ ggml_tensor_overhead(),
+        /*.mem_buffer =*/ NULL,
+        /*.no_alloc   =*/ true,
+    };
+    ggml_context_ptr ctx_ptr { ggml_init(params) };
+    GGML_ASSERT(ctx_ptr != nullptr);
+    ggml_context * ctx = ctx_ptr.get();
+    ggml_tensor * tensor = deserialize_tensor(ctx, &request.tensor);
+    if (tensor == nullptr) {
+        GGML_LOG_ERROR("[%s] error deserializing tensor\n", __func__);
+        return false;
+    }
+    GGML_PRINT_DEBUG("[%s] buffer: %p, data: %p, offset: %" PRIu64 ", size: %" PRIu64 "\n", __func__, (void*)tensor->buffer, tensor->data, request.offset, request.size);
+
+    // sanitize tensor->data
+    {
+        const size_t p0 = (size_t) ggml_backend_buffer_get_base(tensor->buffer);
+        const size_t p1 = p0 + ggml_backend_buffer_get_size(tensor->buffer);
+
+        if (request.tensor.data + request.offset < p0 ||
+            request.tensor.data + request.offset >= p1 ||
+            request.size > (p1 - request.tensor.data - request.offset)) {
+                GGML_LOG_ERROR("[%s] requested tensor region (data=0x%" PRIx64 ", offset=%" PRIu64 ", size=%" PRIu64 ") out of buffer bounds [0x%zx, 0x%zx)\n",
+                               __func__, request.tensor.data, request.offset, request.size, p0, p1);
+                return false;
+        }
+    }
+
+    response.resize(request.size, 0);
+    ggml_backend_tensor_get(tensor, response.data(), request.offset, request.size);
+    return true;
+}
+
+bool rpc_server::copy_tensor(const rpc_msg_copy_tensor_req & request, rpc_msg_copy_tensor_rsp & response) {
+    struct ggml_init_params params {
+        /*.mem_size   =*/ 2*ggml_tensor_overhead(),
+        /*.mem_buffer =*/ NULL,
+        /*.no_alloc   =*/ true,
+    };
+    ggml_context_ptr ctx_ptr { ggml_init(params) };
+    GGML_ASSERT(ctx_ptr != nullptr);
+    ggml_context * ctx = ctx_ptr.get();
+
+    ggml_tensor * src = deserialize_tensor(ctx, &request.src);
+    ggml_tensor * dst = deserialize_tensor(ctx, &request.dst);
+    if (src == nullptr || dst == nullptr) {
+        GGML_LOG_ERROR("[%s] error deserializing tensors\n", __func__);
+        return false;
+    }
+
+    uint64_t src_size   = (uint64_t) ggml_nbytes(src);
+    uint64_t dst_data   = (uint64_t) dst->data;
+    uint64_t dst_base   = (uint64_t) ggml_backend_buffer_get_base(dst->buffer);
+    uint64_t dst_buf_sz = (uint64_t) ggml_backend_buffer_get_size(dst->buffer);
+
+    if (dst_data + src_size > dst_base + dst_buf_sz) {
+        GGML_PRINT_DEBUG("[%s] out-of-bounds write in rpc_server::copy_tensor:\n"
+                         "    write range : [0x%" PRIx64 ", 0x%" PRIx64 "]\n"
+                         "    buffer base: [0x%" PRIx64 ", 0x%" PRIx64 "]\n",
+                         __func__,
+                         dst_data,
+                         dst_data + src_size,
+                         dst_base,
+                         dst_base + dst_buf_sz);
+        return false;
+    }
+
+    GGML_PRINT_DEBUG("[%s] src->buffer: %p, dst->buffer: %p\n",
+                     __func__, (void*) src->buffer, (void*) dst->buffer);
+
+    response.result = ggml_backend_buffer_copy_tensor(src, dst);
+    return true;
+}
+
+ggml_tensor * rpc_server::create_node(uint64_t id,
+                                      struct ggml_context * ctx,
+                                      const std::unordered_map<uint64_t, const rpc_tensor*> & tensor_ptrs,
+                                      std::unordered_map<uint64_t, struct ggml_tensor*> & tensor_map) {
+    if (tensor_map.find(id) != tensor_map.end()) {
+        return tensor_map[id];
+    }
+    // Safely find the tensor pointer
+    auto it_ptr = tensor_ptrs.find(id);
+    if (it_ptr == tensor_ptrs.end()) {
+        return nullptr;
+    }
+    const rpc_tensor * tensor = it_ptr->second;
+
+    struct ggml_tensor * result = deserialize_tensor(ctx, tensor);
+    if (result == nullptr) {
+        return nullptr;
+    }
+    tensor_map[id] = result;
+    for (int i = 0; i < GGML_MAX_SRC; i++) {
+        // Check if the source ID is 0 before calling create_node recursively
+        if (tensor->src[i] == 0) {
+            result->src[i] = nullptr;
+        } else {
+            result->src[i] = create_node(tensor->src[i], ctx, tensor_ptrs, tensor_map);
+            // If the recursive call failed for a non-zero ID, propagate the error
+            if (result->src[i] == nullptr) {
+                GGML_LOG_ERROR("[%s] failed to create source node %d (src_id=%" PRIu64 ") for node id %" PRIu64 "\n",
+                               __func__, i, tensor->src[i], id);
+                // Must return nullptr to signal failure up the call stack
+                return nullptr;
+            }
+        }
+    }
+
+    // Handle view_src similarly
+    if (tensor->view_src == 0) {
+        result->view_src = nullptr;
+    } else {
+        result->view_src = create_node(tensor->view_src, ctx, tensor_ptrs, tensor_map);
+        // If the recursive call failed for a non-zero ID, propagate the error
+        if (result->view_src == nullptr) {
+            GGML_LOG_ERROR("[%s] failed to create view_src node (view_src_id=%" PRIu64 ") for node id %" PRIu64 "\n",
+                           __func__, tensor->view_src, id);
+            // Must return nullptr to signal failure up the call stack
+            return nullptr;
+        }
+    }
+    result->view_offs = tensor->view_offs;
+    return result;
+}
+
+bool rpc_server::graph_compute(const std::vector<uint8_t> & input, rpc_msg_graph_compute_rsp & response) {
+    // serialization format:
+    // | n_nodes (4 bytes) | nodes (n_nodes * sizeof(uint64_t) | n_tensors (4 bytes) | tensors (n_tensors * sizeof(rpc_tensor)) |
+    if (input.size() < sizeof(uint32_t)) {
+        return false;
+    }
+    uint32_t n_nodes;
+    memcpy(&n_nodes, input.data(), sizeof(n_nodes));
+    if (input.size() < sizeof(uint32_t) + n_nodes*sizeof(uint64_t) + sizeof(uint32_t)) {
+        return false;
+    }
+    const uint64_t * nodes = (const uint64_t *)(input.data() + sizeof(n_nodes));
+    uint32_t n_tensors;
+    memcpy(&n_tensors, input.data() + sizeof(n_nodes) + n_nodes*sizeof(uint64_t), sizeof(n_tensors));
+    if (input.size() < sizeof(uint32_t) + n_nodes*sizeof(uint64_t) + sizeof(uint32_t) + n_tensors*sizeof(rpc_tensor)) {
+        return false;
+    }
+    const rpc_tensor * tensors = (const rpc_tensor *)(input.data() + sizeof(n_nodes) + n_nodes*sizeof(uint64_t) + sizeof(n_tensors));
+    GGML_PRINT_DEBUG("[%s] n_nodes: %u, n_tensors: %u\n", __func__, n_nodes, n_tensors);
+
+    size_t buf_size = ggml_tensor_overhead()*(n_nodes + n_tensors) + ggml_graph_overhead_custom(n_nodes, false);
+
+    struct ggml_init_params params = {
+        /*.mem_size   =*/ buf_size,
+        /*.mem_buffer =*/ NULL,
+        /*.no_alloc   =*/ true,
+    };
+    ggml_context_ptr ctx_ptr { ggml_init(params) };
+    GGML_ASSERT(ctx_ptr != nullptr);
+    ggml_context * ctx = ctx_ptr.get();
+    struct ggml_cgraph * graph = ggml_new_graph_custom(ctx, n_nodes, false);
+    graph->n_nodes = n_nodes;
+    std::unordered_map<uint64_t, const rpc_tensor*> tensor_ptrs;
+    for (uint32_t i = 0; i < n_tensors; i++) {
+        tensor_ptrs[tensors[i].id] = &tensors[i];
+    }
+    std::unordered_map<uint64_t, ggml_tensor*> tensor_map;
+    for (uint32_t i = 0; i < n_nodes; i++) {
+        int64_t id;
+        memcpy(&id, &nodes[i], sizeof(id));
+        graph->nodes[i] = create_node(id, ctx, tensor_ptrs, tensor_map);
+
+        // Check if create_node failed for a *non-zero* ID.
+        // If id was 0, create_node returning nullptr is expected.
+        // If id was non-zero and create_node returned nullptr, it indicates a deserialization error.
+        if (graph->nodes[i] == nullptr && id != 0) {
+            GGML_LOG_ERROR("[%s] failed to create graph node %d (id=%" PRId64 ")\n", __func__, i, id);
+            return false;
+        }
+    }
+    ggml_status status = ggml_backend_graph_compute(backend, graph);
+    response.result = status;
+    return true;
+}
+
+rpc_server::~rpc_server() {
+    for (auto buffer : buffers) {
+        ggml_backend_buffer_free(buffer);
+    }
+}
+
+static void rpc_serve_client(ggml_backend_t backend, const char * cache_dir,
+                             sockfd_t sockfd, size_t free_mem, size_t total_mem) {
+    rpc_server server(backend, cache_dir);
+    uint8_t cmd;
+    if (!recv_data(sockfd, &cmd, 1)) {
+        return;
+    }
+    // the first command sent by the client must be HELLO
+    if (cmd != RPC_CMD_HELLO) {
+        fprintf(stderr, "Expected HELLO command, update client\n");
+        return;
+    }
+    if (!recv_msg(sockfd, nullptr, 0)) {
+        return;
+    }
+    rpc_msg_hello_rsp response;
+    server.hello(response);
+    if (!send_msg(sockfd, &response, sizeof(response))) {
+        return;
+    }
+    while (true) {
+        if (!recv_data(sockfd, &cmd, 1)) {
+            break;
+        }
+        if (cmd >= RPC_CMD_COUNT) {
+            // fail fast if the command is invalid
+            fprintf(stderr, "Unknown command: %d\n", cmd);
+            break;
+        }
+        switch (cmd) {
+            case RPC_CMD_HELLO: {
+                // HELLO command is handled above
+                return;
+            }
+            case RPC_CMD_ALLOC_BUFFER: {
+                rpc_msg_alloc_buffer_req request;
+                if (!recv_msg(sockfd, &request, sizeof(request))) {
+                    return;
+                }
+                rpc_msg_alloc_buffer_rsp response;
+                server.alloc_buffer(request, response);
+                if (!send_msg(sockfd, &response, sizeof(response))) {
+                    return;
+                }
+                break;
+            }
+            case RPC_CMD_GET_ALLOC_SIZE: {
+                rpc_msg_get_alloc_size_req request;
+                if (!recv_msg(sockfd, &request, sizeof(request))) {
+                    return;
+                }
+                rpc_msg_get_alloc_size_rsp response;
+                if (!server.get_alloc_size(request, response)) {
+                    return;
+                }
+                if (!send_msg(sockfd, &response, sizeof(response))) {
+                    return;
+                }
+                break;
+            }
+            case RPC_CMD_GET_ALIGNMENT: {
+                if (!recv_msg(sockfd, nullptr, 0)) {
+                    return;
+                }
+                rpc_msg_get_alignment_rsp response;
+                server.get_alignment(response);
+                if (!send_msg(sockfd, &response, sizeof(response))) {
+                    return;
+                }
+                break;
+            }
+            case RPC_CMD_GET_MAX_SIZE: {
+                if (!recv_msg(sockfd, nullptr, 0)) {
+                    return;
+                }
+                rpc_msg_get_max_size_rsp response;
+                server.get_max_size(response);
+                if (!send_msg(sockfd, &response, sizeof(response))) {
+                    return;
+                }
+                break;
+            }
+            case RPC_CMD_BUFFER_GET_BASE: {
+                rpc_msg_buffer_get_base_req request;
+                if (!recv_msg(sockfd, &request, sizeof(request))) {
+                    return;
+                }
+                rpc_msg_buffer_get_base_rsp response;
+                if (!server.buffer_get_base(request, response)) {
+                    return;
+                }
+                if (!send_msg(sockfd, &response, sizeof(response))) {
+                    return;
+                }
+                break;
+            }
+            case RPC_CMD_FREE_BUFFER: {
+                rpc_msg_free_buffer_req request;
+                if (!recv_msg(sockfd, &request, sizeof(request))) {
+                    return;
+                }
+                if (!server.free_buffer(request)) {
+                    return;
+                }
+                if (!send_msg(sockfd, nullptr, 0)) {
+                    return;
+                }
+                break;
+            }
+            case RPC_CMD_BUFFER_CLEAR: {
+                rpc_msg_buffer_clear_req request;
+                if (!recv_msg(sockfd, &request, sizeof(request))) {
+                    return;
+                }
+                if (!server.buffer_clear(request)) {
+                    return;
+                }
+                if (!send_msg(sockfd, nullptr, 0)) {
+                    return;
+                }
+                break;
+            }
+            case RPC_CMD_SET_TENSOR: {
+                std::vector<uint8_t> input;
+                if (!recv_msg(sockfd, input)) {
+                    return;
+                }
+                if (!server.set_tensor(input)) {
+                    return;
+                }
+                break;
+            }
+            case RPC_CMD_SET_TENSOR_HASH: {
+                std::vector<uint8_t> input;
+                if (!recv_msg(sockfd, input)) {
+                    return;
+                }
+                rpc_msg_set_tensor_hash_rsp response;
+                if (!server.set_tensor_hash(input, response)) {
+                    return;
+                }
+                if (!send_msg(sockfd, &response, sizeof(response))) {
+                    return;
+                }
+                break;
+            }
+            case RPC_CMD_INIT_TENSOR: {
+                rpc_msg_init_tensor_req request;
+                if (!recv_msg(sockfd, &request,sizeof(request))) {
+                    return;
+                }
+                if (!server.init_tensor(request)) {
+                    return;
+                }
+                if (!send_msg(sockfd, nullptr, 0)) {
+                    return;
+                }
+                break;
+            }
+            case RPC_CMD_GET_TENSOR: {
+                rpc_msg_get_tensor_req request;
+                if (!recv_msg(sockfd, &request, sizeof(request))) {
+                    return;
+                }
+                std::vector<uint8_t> response;
+                if (!server.get_tensor(request, response)) {
+                    return;
+                }
+                if (!send_msg(sockfd, response.data(), response.size())) {
+                    return;
+                }
+                break;
+            }
+            case RPC_CMD_COPY_TENSOR: {
+                rpc_msg_copy_tensor_req request;
+                if (!recv_msg(sockfd, &request, sizeof(request))) {
+                    return;
+                }
+                rpc_msg_copy_tensor_rsp response;
+                if (!server.copy_tensor(request, response)) {
+                    return;
+                }
+                if (!send_msg(sockfd, &response, sizeof(response))) {
+                    return;
+                }
+                break;
+            }
+            case RPC_CMD_GRAPH_COMPUTE: {
+                std::vector<uint8_t> input;
+                if (!recv_msg(sockfd, input)) {
+                    return;
+                }
+                rpc_msg_graph_compute_rsp response;
+                if (!server.graph_compute(input, response)) {
+                    return;
+                }
+                if (!send_msg(sockfd, &response, sizeof(response))) {
+                    return;
+                }
+                break;
+            }
+            case RPC_CMD_GET_DEVICE_MEMORY: {
+                if (!recv_msg(sockfd, nullptr, 0)) {
+                    return;
+                }
+                rpc_msg_get_device_memory_rsp response;
+                response.free_mem = free_mem;
+                response.total_mem = total_mem;
+                if (!send_msg(sockfd, &response, sizeof(response))) {
+                    return;
+                }
+                break;
+            }
+            default: {
+                fprintf(stderr, "Unknown command: %d\n", cmd);
+                return;
+            }
+        }
+    }
+}
+
+void ggml_backend_rpc_start_server(ggml_backend_t backend, const char * endpoint,
+                                   const char * cache_dir,
+                                   size_t free_mem, size_t total_mem) {
+    std::string host;
+    int port;
+    if (!parse_endpoint(endpoint, host, port)) {
+        return;
+    }
+#ifdef _WIN32
+    {
+        WSADATA wsaData;
+        int res = WSAStartup(MAKEWORD(2, 2), &wsaData);
+        if (res != 0) {
+            fprintf(stderr, "WSAStartup failed: %d\n", res);
+            return;
+        }
+    }
+#endif
+    auto server_socket = create_server_socket(host.c_str(), port);
+    if (server_socket == nullptr) {
+        fprintf(stderr, "Failed to create server socket\n");
+        return;
+    }
+    while (true) {
+        auto client_socket = socket_accept(server_socket->fd);
+        if (client_socket == nullptr) {
+            fprintf(stderr, "Failed to accept client connection\n");
+            return;
+        }
+        printf("Accepted client connection, free_mem=%zu, total_mem=%zu\n", free_mem, total_mem);
+        fflush(stdout);
+        rpc_serve_client(backend, cache_dir, client_socket->fd, free_mem, total_mem);
+        printf("Client connection closed\n");
+        fflush(stdout);
+    }
+#ifdef _WIN32
+    WSACleanup();
+#endif
+}
+
+// device interface
+
+struct ggml_backend_rpc_device_context {
+    std::string endpoint;
+    std::string name;
+};
+
+static const char * ggml_backend_rpc_device_get_name(ggml_backend_dev_t dev) {
+    ggml_backend_rpc_device_context * ctx = (ggml_backend_rpc_device_context *)dev->context;
+
+    return ctx->name.c_str();
+}
+
+static const char * ggml_backend_rpc_device_get_description(ggml_backend_dev_t dev) {
+    ggml_backend_rpc_device_context * ctx = (ggml_backend_rpc_device_context *)dev->context;
+
+    return ctx->name.c_str();
+}
+
+static void ggml_backend_rpc_device_get_memory(ggml_backend_dev_t dev, size_t * free, size_t * total) {
+    ggml_backend_rpc_device_context * ctx = (ggml_backend_rpc_device_context *)dev->context;
+
+    ggml_backend_rpc_get_device_memory(ctx->endpoint.c_str(), free, total);
+
+    GGML_UNUSED(dev);
+}
+
+static enum ggml_backend_dev_type ggml_backend_rpc_device_get_type(ggml_backend_dev_t dev) {
+    // TODO: obtain value from the server
+    return GGML_BACKEND_DEVICE_TYPE_GPU;
+
+    GGML_UNUSED(dev);
+}
+
+static void ggml_backend_rpc_device_get_props(ggml_backend_dev_t dev, struct ggml_backend_dev_props * props) {
+    props->name        = ggml_backend_rpc_device_get_name(dev);
+    props->description = ggml_backend_rpc_device_get_description(dev);
+    props->type        = ggml_backend_rpc_device_get_type(dev);
+    ggml_backend_rpc_device_get_memory(dev, &props->memory_free, &props->memory_total);
+    props->caps = {
+        /* .async                 = */ false,
+        /* .host_buffer           = */ false,
+        /* .buffer_from_host_ptr  = */ false,
+        /* .events                = */ false,
+    };
+}
+
+static ggml_backend_t ggml_backend_rpc_device_init(ggml_backend_dev_t dev, const char * params) {
+    ggml_backend_rpc_device_context * ctx = (ggml_backend_rpc_device_context *)dev->context;
+
+    return ggml_backend_rpc_init(ctx->endpoint.c_str());
+
+    GGML_UNUSED(params);
+}
+
+static ggml_backend_buffer_type_t ggml_backend_rpc_device_get_buffer_type(ggml_backend_dev_t dev) {
+    ggml_backend_rpc_device_context * ctx = (ggml_backend_rpc_device_context *)dev->context;
+
+    return ggml_backend_rpc_buffer_type(ctx->endpoint.c_str());
+
+    GGML_UNUSED(dev);
+}
+
+static bool ggml_backend_rpc_device_supports_op(ggml_backend_dev_t dev, const struct ggml_tensor * op) {
+    GGML_UNUSED(dev);
+    GGML_UNUSED(op);
+    //TODO: call the remote backend and cache the results
+    return true;
+}
+
+static bool ggml_backend_rpc_device_supports_buft(ggml_backend_dev_t dev, ggml_backend_buffer_type_t buft) {
+    if (!buft || buft->iface.get_name != ggml_backend_rpc_buffer_type_name) {
+        return false;
+    }
+    ggml_backend_rpc_buffer_type_context * buft_ctx = (ggml_backend_rpc_buffer_type_context *)buft->context;
+    ggml_backend_rpc_device_context * dev_ctx = (ggml_backend_rpc_device_context *)dev->context;
+    return buft_ctx->endpoint == dev_ctx->endpoint;
+}
+
+static const struct ggml_backend_device_i ggml_backend_rpc_device_i = {
+    /* .get_name             = */ ggml_backend_rpc_device_get_name,
+    /* .get_description      = */ ggml_backend_rpc_device_get_description,
+    /* .get_memory           = */ ggml_backend_rpc_device_get_memory,
+    /* .get_type             = */ ggml_backend_rpc_device_get_type,
+    /* .get_props            = */ ggml_backend_rpc_device_get_props,
+    /* .init_backend         = */ ggml_backend_rpc_device_init,
+    /* .get_buffer_type      = */ ggml_backend_rpc_device_get_buffer_type,
+    /* .get_host_buffer_type = */ NULL,
+    /* .buffer_from_host_ptr = */ NULL,
+    /* .supports_op          = */ ggml_backend_rpc_device_supports_op,
+    /* .supports_buft        = */ ggml_backend_rpc_device_supports_buft,
+    /* .offload_op           = */ NULL,
+    /* .event_new            = */ NULL,
+    /* .event_free           = */ NULL,
+    /* .event_synchronize    = */ NULL,
+};
+
+// backend reg interface
+
+static const char * ggml_backend_rpc_reg_get_name(ggml_backend_reg_t reg) {
+    return "RPC";
+
+    GGML_UNUSED(reg);
+}
+
+static size_t ggml_backend_rpc_reg_get_device_count(ggml_backend_reg_t reg) {
+    return 0;
+
+    GGML_UNUSED(reg);
+}
+
+static ggml_backend_dev_t ggml_backend_rpc_reg_get_device(ggml_backend_reg_t reg, size_t index) {
+    GGML_ABORT("The RPC backend does not have enumerated devices - use ggml_backend_add_device instead");
+
+    GGML_UNUSED(reg);
+    GGML_UNUSED(index);
+}
+
+static void * ggml_backend_rpc_get_proc_address(ggml_backend_reg_t reg, const char * name) {
+    if (std::strcmp(name, "ggml_backend_rpc_add_device") == 0) {
+        return (void *)ggml_backend_rpc_add_device;
+    }
+    return NULL;
+
+    GGML_UNUSED(reg);
+}
+
+static const struct ggml_backend_reg_i ggml_backend_rpc_reg_i = {
+    /* .get_name         = */ ggml_backend_rpc_reg_get_name,
+    /* .get_device_count = */ ggml_backend_rpc_reg_get_device_count,
+    /* .get_device       = */ ggml_backend_rpc_reg_get_device,
+    /* .get_proc_address = */ ggml_backend_rpc_get_proc_address,
+};
+
+ggml_backend_reg_t ggml_backend_rpc_reg(void) {
+    static struct ggml_backend_reg ggml_backend_rpc_reg = {
+        /* .api_version = */ GGML_BACKEND_API_VERSION,
+        /* .iface       = */ ggml_backend_rpc_reg_i,
+        /* .context     = */ NULL,
+    };
+
+    return &ggml_backend_rpc_reg;
+}
+
+ggml_backend_dev_t ggml_backend_rpc_add_device(const char * endpoint) {
+    static std::unordered_map<std::string, ggml_backend_dev_t> dev_map;
+
+    static std::mutex mutex;
+    std::lock_guard<std::mutex> lock(mutex);
+
+    if (dev_map.find(endpoint) != dev_map.end()) {
+        return dev_map[endpoint];
+    }
+
+    ggml_backend_rpc_device_context * ctx = new ggml_backend_rpc_device_context {
+        /* .endpoint = */ endpoint,
+        /* .name     = */ "RPC[" + std::string(endpoint) + "]",
+    };
+
+    ggml_backend_dev_t dev = new ggml_backend_device {
+        /* .iface   = */ ggml_backend_rpc_device_i,
+        /* .reg     = */ ggml_backend_rpc_reg(),
+        /* .context = */ ctx,
+    };
+
+    dev_map[endpoint] = dev;
+
+    return dev;
+}
+
+GGML_BACKEND_DL_IMPL(ggml_backend_rpc_reg)
diff --git a/ml/backend/ggml/ggml/src/ggml-rpc/rpc-server.cpp b/ml/backend/ggml/ggml/src/ggml-rpc/rpc-server.cpp
new file mode 100644
index 00000000..ea758028
--- /dev/null
+++ b/ml/backend/ggml/ggml/src/ggml-rpc/rpc-server.cpp
@@ -0,0 +1,326 @@
+#if defined(_MSC_VER)
+#define _SILENCE_CXX17_CODECVT_HEADER_DEPRECATION_WARNING
+#endif
+
+#include "ggml-rpc.h"
+#ifdef _WIN32
+#  define NOMINMAX
+#  define DIRECTORY_SEPARATOR '\\'
+#  include <locale>
+#  include <windows.h>
+#  include <fcntl.h>
+#  include <io.h>
+#else
+#  define DIRECTORY_SEPARATOR '/'
+#  include <unistd.h>
+#  include <sys/stat.h>
+#endif
+#include <codecvt>
+#include <string>
+#include <stdio.h>
+#include <vector>
+#include <filesystem>
+#include <algorithm>
+#include <thread>
+#include "rpc-server.h"
+
+namespace fs = std::filesystem;
+
+struct rpc_server_params {
+    std::string host        = "127.0.0.1";
+    int         port        = 50052;
+    size_t      backend_mem = 0;
+    bool        use_cache   = true;
+    int         n_threads   = std::max(1U, std::thread::hardware_concurrency()/2);
+    std::string device;
+};
+
+// NOTE: this is copied from common.cpp to avoid linking with libcommon
+// returns true if successful, false otherwise
+static bool fs_create_directory_with_parents(const std::string & path) {
+#ifdef _WIN32
+    std::wstring_convert<std::codecvt_utf8<wchar_t>> converter;
+    std::wstring wpath = converter.from_bytes(path);
+
+    // if the path already exists, check whether it's a directory
+    const DWORD attributes = GetFileAttributesW(wpath.c_str());
+    if ((attributes != INVALID_FILE_ATTRIBUTES) && (attributes & FILE_ATTRIBUTE_DIRECTORY)) {
+        return true;
+    }
+
+    size_t pos_slash = 0;
+
+    // process path from front to back, procedurally creating directories
+    while ((pos_slash = path.find('\\', pos_slash)) != std::string::npos) {
+        const std::wstring subpath = wpath.substr(0, pos_slash);
+        const wchar_t * test = subpath.c_str();
+
+        const bool success = CreateDirectoryW(test, NULL);
+        if (!success) {
+            const DWORD error = GetLastError();
+
+            // if the path already exists, ensure that it's a directory
+            if (error == ERROR_ALREADY_EXISTS) {
+                const DWORD attributes = GetFileAttributesW(subpath.c_str());
+                if (attributes == INVALID_FILE_ATTRIBUTES || !(attributes & FILE_ATTRIBUTE_DIRECTORY)) {
+                    return false;
+                }
+            } else {
+                return false;
+            }
+        }
+
+        pos_slash += 1;
+    }
+
+    return true;
+#else
+    // if the path already exists, check whether it's a directory
+    struct stat info;
+    if (stat(path.c_str(), &info) == 0) {
+        return S_ISDIR(info.st_mode);
+    }
+
+    size_t pos_slash = 1; // skip leading slashes for directory creation
+
+    // process path from front to back, procedurally creating directories
+    while ((pos_slash = path.find('/', pos_slash)) != std::string::npos) {
+        const std::string subpath = path.substr(0, pos_slash);
+        struct stat info;
+
+        // if the path already exists, ensure that it's a directory
+        if (stat(subpath.c_str(), &info) == 0) {
+            if (!S_ISDIR(info.st_mode)) {
+                return false;
+            }
+        } else {
+            // create parent directories
+            const int ret = mkdir(subpath.c_str(), 0755);
+            if (ret != 0) {
+                return false;
+            }
+        }
+
+        pos_slash += 1;
+    }
+
+    return true;
+#endif // _WIN32
+}
+
+// NOTE: this is copied from common.cpp to avoid linking with libcommon
+static std::string fs_get_cache_directory() {
+    std::string cache_directory = "";
+    auto ensure_trailing_slash = [](std::string p) {
+        // Make sure to add trailing slash
+        if (p.back() != DIRECTORY_SEPARATOR) {
+            p += DIRECTORY_SEPARATOR;
+        }
+        return p;
+    };
+    if (getenv("OLLAMA_CACHE")) {
+        cache_directory = std::getenv("LLAMA_CACHE");
+    } else {
+#if defined(__linux__) || defined(__FreeBSD__) || defined(_AIX) || defined(__APPLE__)
+        cache_directory = std::getenv("HOME");
+#elif defined(_WIN32)
+        cache_directory = std::getenv("USERPROFILE");
+#else
+#  error Unknown architecture
+#endif
+        cache_directory = ensure_trailing_slash(cache_directory);
+        cache_directory += ".ollama";
+    }
+    return ensure_trailing_slash(cache_directory);
+}
+
+static void print_usage(int /*argc*/, char ** argv, rpc_server_params params) {
+    fprintf(stderr, "Usage: %s [options]\n\n", argv[0]);
+    fprintf(stderr, "options:\n");
+    fprintf(stderr, "  -h, --help                show this help message and exit\n");
+    fprintf(stderr, "  -t,      --threads        number of threads for the CPU backend (default: %d)\n", params.n_threads);
+    fprintf(stderr, "  -d DEV,  --device         device to use\n");
+    fprintf(stderr, "  -H HOST, --host HOST      host to bind to (default: %s)\n", params.host.c_str());
+    fprintf(stderr, "  -p PORT, --port PORT      port to bind to (default: %d)\n", params.port);
+    fprintf(stderr, "  -m MEM,  --mem MEM        backend memory size (in MB)\n");
+    fprintf(stderr, "  -c,      --cache          enable local file cache\n");
+    fprintf(stderr, "\n");
+}
+
+static bool rpc_server_params_parse(int argc, char ** argv, rpc_server_params & params) {
+    std::string arg;
+    for (int i = 1; i < argc; i++) {
+        arg = argv[i];
+        if (arg == "-H" || arg == "--host") {
+            if (++i >= argc) {
+                return false;
+            }
+            params.host = argv[i];
+        } else if (arg == "-t" || arg == "--threads") {
+            if (++i >= argc) {
+                return false;
+            }
+            params.n_threads = std::stoi(argv[i]);
+            if (params.n_threads <= 0) {
+                fprintf(stderr, "error: invalid number of threads: %d\n", params.n_threads);
+                return false;
+            }
+        } else if (arg == "-d" || arg == "--device") {
+            if (++i >= argc) {
+                return false;
+            }
+            params.device = argv[i];
+            if (ggml_backend_dev_by_name(params.device.c_str()) == nullptr) {
+                fprintf(stderr, "error: unknown device: %s\n", params.device.c_str());
+                fprintf(stderr, "available devices:\n");
+                for (size_t i = 0; i < ggml_backend_dev_count(); i++) {
+                    auto * dev = ggml_backend_dev_get(i);
+                    size_t free, total;
+                    ggml_backend_dev_memory(dev, &free, &total);
+                    printf("  %s: %s (%zu MiB, %zu MiB free)\n", ggml_backend_dev_name(dev), ggml_backend_dev_description(dev), total / 1024 / 1024, free / 1024 / 1024);
+                }
+                return false;
+            }
+        } else if (arg == "-p" || arg == "--port") {
+            if (++i >= argc) {
+                return false;
+            }
+            params.port = std::stoi(argv[i]);
+            if (params.port <= 0 || params.port > 65535) {
+                return false;
+            }
+        } else if (arg == "-c" || arg == "--cache") {
+            params.use_cache = true;
+        } else if (arg == "-m" || arg == "--mem") {
+            if (++i >= argc) {
+                return false;
+            }
+            params.backend_mem = std::stoul(argv[i]) * 1024 * 1024;
+        } else if (arg == "-h" || arg == "--help") {
+            print_usage(argc, argv, params);
+            exit(0);
+        } else {
+            fprintf(stderr, "error: unknown argument: %s\n", arg.c_str());
+            print_usage(argc, argv, params);
+            exit(0);
+        }
+    }
+    return true;
+}
+
+static ggml_backend_t create_backend(const rpc_server_params & params) {
+    ggml_backend_t backend = nullptr;
+
+    if (!params.device.empty()) {
+        ggml_backend_dev_t dev = ggml_backend_dev_by_name(params.device.c_str());
+        if (dev) {
+            backend = ggml_backend_dev_init(dev, nullptr);
+            if (!backend) {
+                fprintf(stderr, "Failed to create backend for device %s\n", params.device.c_str());
+                return nullptr;
+            }
+        }
+    }
+
+    // try to initialize a GPU backend first
+    if (!backend) {
+        backend = ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_GPU, nullptr);
+    }
+
+    // if there aren't GPU backends fallback to CPU backend
+    if (!backend) {
+        backend = ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_CPU, nullptr);
+    }
+
+    if (backend) {
+        fprintf(stderr, "%s: using %s backend\n", __func__, ggml_backend_name(backend));
+
+        // set the number of threads
+        ggml_backend_dev_t dev = ggml_backend_get_device(backend);
+        ggml_backend_reg_t reg = dev ? ggml_backend_dev_backend_reg(dev) : nullptr;
+        if (reg) {
+            auto ggml_backend_set_n_threads_fn = (ggml_backend_set_n_threads_t) ggml_backend_reg_get_proc_address(reg, "ggml_backend_set_n_threads");
+            if (ggml_backend_set_n_threads_fn) {
+                ggml_backend_set_n_threads_fn(backend, params.n_threads);
+            }
+        }
+    }
+
+    return backend;
+}
+
+static void get_backend_memory(ggml_backend_t backend, size_t * free_mem, size_t * total_mem) {
+    ggml_backend_dev_t dev = ggml_backend_get_device(backend);
+    GGML_ASSERT(dev != nullptr);
+    ggml_backend_dev_memory(dev, free_mem, total_mem);
+}
+
+int run_rpc_server(const char *host, int port, const char *device) {
+    rpc_server_params params;
+    params.host = host;
+    params.port = port;
+    params.device = device;
+
+    ggml_backend_load_all();
+
+    if (params.host != "127.0.0.1") {
+        fprintf(stderr, "\n");
+        fprintf(stderr, "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n");
+        fprintf(stderr, "WARNING: Host ('%s') is != '127.0.0.1'\n", params.host.c_str());
+        fprintf(stderr, "         Never expose the RPC server to an open network!\n");
+        fprintf(stderr, "         This is an experimental feature and is not secure!\n");
+        fprintf(stderr, "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n");
+        fprintf(stderr, "\n");
+    }
+
+     if (!params.device.empty()) {
+        if (ggml_backend_dev_by_name(params.device.c_str()) == nullptr) {
+            fprintf(stderr, "error: unknown device: %s\n", params.device.c_str());
+            fprintf(stderr, "available devices:\n");
+            for (size_t i = 0; i < ggml_backend_dev_count(); i++) {
+                auto *dev = ggml_backend_dev_get(i);
+                size_t free, total;
+                ggml_backend_dev_memory(dev, &free, &total);
+                printf("  %s: %s (%zu MiB, %zu MiB free)\n", ggml_backend_dev_name(dev),
+                ggml_backend_dev_description(dev), total / 1024 / 1024,
+                free / 1024 / 1024);
+            }
+            return false;
+        }
+    }
+
+    ggml_backend_t backend = create_backend(params);
+    if (!backend) {
+        fprintf(stderr, "Failed to create backend\n");
+        return 1;
+    }
+    std::string endpoint = params.host + ":" + std::to_string(params.port);
+    size_t free_mem, total_mem;
+    if (params.backend_mem > 0) {
+        free_mem = params.backend_mem;
+        total_mem = params.backend_mem;
+    } else {
+        get_backend_memory(backend, &free_mem, &total_mem);
+    }
+    const char * cache_dir = nullptr;
+    std::string cache_dir_str;
+    if (params.use_cache) {
+        cache_dir_str = fs_get_cache_directory() + "rpc/";
+        if (!fs_create_directory_with_parents(cache_dir_str)) {
+            fprintf(stderr, "Failed to create cache directory: %s\n", cache_dir_str.c_str());
+            return 1;
+        }
+        cache_dir = cache_dir_str.c_str();
+    }
+
+    ggml_backend_reg_t reg = ggml_backend_reg_by_name("RPC");
+    if (!reg) {
+        fprintf(stderr, "Failed to find RPC backend\n");
+        return 1;
+    }
+
+    ggml_backend_rpc_start_server(backend, endpoint.c_str(), cache_dir, free_mem, total_mem);
+
+    ggml_backend_free(backend);
+    return 0;
+}
\ No newline at end of file
diff --git a/ml/backend/ggml/ggml/src/ggml-rpc/rpc.go b/ml/backend/ggml/ggml/src/ggml-rpc/rpc.go
new file mode 100644
index 00000000..f89d47dd
--- /dev/null
+++ b/ml/backend/ggml/ggml/src/ggml-rpc/rpc.go
@@ -0,0 +1,6 @@
+package rpc
+
+// #cgo CPPFLAGS: -I${SRCDIR}/../../include
+// #cgo CPPFLAGS: -I${SRCDIR}/../
+// #cgo CXXFLAGS: -std=c++17
+import "C"
diff --git a/ml/backend/ggml/ggml/src/ggml.go b/ml/backend/ggml/ggml/src/ggml.go
index 91f1f1ad..cd861991 100644
--- a/ml/backend/ggml/ggml/src/ggml.go
+++ b/ml/backend/ggml/ggml/src/ggml.go
@@ -1,7 +1,7 @@
 package ggml
 
 // #cgo CXXFLAGS: -std=c++17
-// #cgo CPPFLAGS: -DNDEBUG -DGGML_USE_CPU
+// #cgo CPPFLAGS: -DNDEBUG -DGGML_USE_CPU -DGGML_USE_RPC
 // #cgo CPPFLAGS: -I${SRCDIR}/../include -I${SRCDIR}/ggml-cpu
 // #cgo windows CFLAGS: -Wno-dll-attribute-on-redeclaration
 // #cgo windows LDFLAGS: -lmsvcrt -static -static-libgcc -static-libstdc++
@@ -37,6 +37,7 @@ import (
 	"unsafe"
 
 	_ "github.com/ollama/ollama/ml/backend/ggml/ggml/src/ggml-cpu"
+	_ "github.com/ollama/ollama/ml/backend/ggml/ggml/src/ggml-rpc"
 )
 
 func init() {
diff --git a/ml/backend/ggml/utils.cpp b/ml/backend/ggml/utils.cpp
new file mode 100644
index 00000000..48fe38d6
--- /dev/null
+++ b/ml/backend/ggml/utils.cpp
@@ -0,0 +1,48 @@
+#include <stdexcept>
+#include <string>
+#include <vector>
+
+#include "ggml-backend.h"
+#include "ggml-rpc.h"
+
+#include "utils.h"
+
+std::vector<std::string> string_split(const std::string & input, char separator)
+{
+    std::vector<std::string> parts;
+    size_t begin_pos = 0;
+    size_t separator_pos = input.find(separator);
+    while (separator_pos != std::string::npos) {
+        std::string part = input.substr(begin_pos, separator_pos - begin_pos);
+        parts.emplace_back(part);
+        begin_pos = separator_pos + 1;
+        separator_pos = input.find(separator, begin_pos);
+    }
+    parts.emplace_back(input.substr(begin_pos, separator_pos - begin_pos));
+    return parts;
+}
+
+void add_rpc_devices(const char* const input_servers) {
+    std::string servers = input_servers;
+    auto rpc_servers = string_split(servers, ',');
+    if (rpc_servers.empty()) {
+        throw std::invalid_argument("no RPC servers specified");
+    }
+    ggml_backend_reg_t rpc_reg = ggml_backend_reg_by_name("RPC");
+    if (!rpc_reg) {
+        throw std::invalid_argument("failed to find RPC backend");
+    }
+    typedef ggml_backend_dev_t (*ggml_backend_rpc_add_device_t)(const char * endpoint);
+    ggml_backend_rpc_add_device_t ggml_backend_rpc_add_device_fn = (ggml_backend_rpc_add_device_t) ggml_backend_reg_get_proc_address(rpc_reg, "ggml_backend_rpc_add_device");
+    if (!ggml_backend_rpc_add_device_fn) {
+        throw std::invalid_argument("failed to find RPC device add function");
+    }
+    for (const auto & server : rpc_servers) {
+        ggml_backend_dev_t dev = ggml_backend_rpc_add_device_fn(server.c_str());
+        if (dev) {
+            ggml_backend_device_register(dev);
+        } else {
+            throw std::invalid_argument("failed to register RPC device");
+        }
+    }
+}
diff --git a/ml/backend/ggml/utils.h b/ml/backend/ggml/utils.h
new file mode 100644
index 00000000..02af0a36
--- /dev/null
+++ b/ml/backend/ggml/utils.h
@@ -0,0 +1,16 @@
+// TODO: this is a temporary wrapper to add RPC servers
+#ifndef UTILS_H
+#define UTILS_H
+
+#ifdef __cplusplus
+extern "C"
+{
+#endif
+
+    void add_rpc_devices(const char* const input_servers);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif // UTILS_H
diff --git a/runner/llamarunner/runner.go b/runner/llamarunner/runner.go
index 7aa9b96a..fd6d1232 100644
--- a/runner/llamarunner/runner.go
+++ b/runner/llamarunner/runner.go
@@ -800,6 +800,7 @@ func Execute(args []string) error {
 	noMmap := fs.Bool("no-mmap", false, "do not memory-map model (slower load but may reduce pageouts if not using mlock)")
 	tensorSplit := fs.String("tensor-split", "", "fraction of the model to offload to each GPU, comma-separated list of proportions")
 	multiUserCache := fs.Bool("multiuser-cache", false, "optimize input cache algorithm for multiple users")
+	rpcServers := fs.String("rpc", "", "comma separated list of RPC servers")
 
 	var lpaths multiLPath
 	fs.Var(&lpaths, "lora", "Path to lora layer file (can be specified multiple times)")
@@ -839,6 +840,7 @@ func Execute(args []string) error {
 		MainGpu:      *mainGpu,
 		UseMmap:      !*noMmap && lpaths.String() == "",
 		TensorSplit:  tensorSplitFloats,
+		RPCServers:   *rpcServers,
 		Progress: func(progress float32) {
 			server.progress = progress
 		},
diff --git a/runner/ollamarunner/runner.go b/runner/ollamarunner/runner.go
index a7a889f1..f424574f 100644
--- a/runner/ollamarunner/runner.go
+++ b/runner/ollamarunner/runner.go
@@ -910,6 +910,7 @@ func Execute(args []string) error {
 	_ = fs.Bool("no-mmap", false, "do not memory-map model (slower load but may reduce pageouts if not using mlock)")
 	tensorSplit := fs.String("tensor-split", "", "fraction of the model to offload to each GPU, comma-separated list of proportions")
 	multiUserCache := fs.Bool("multiuser-cache", false, "optimize input cache algorithm for multiple users")
+	rpcServers := fs.String("rpc", "", "comma separated list of RPC servers")
 
 	var lpaths multiLPath
 	fs.Var(&lpaths, "lora", "Path to lora layer file (can be specified multiple times)")
@@ -954,6 +955,7 @@ func Execute(args []string) error {
 		MainGPU:        *mainGPU,
 		TensorSplit:    tensorSplitFloats,
 		FlashAttention: *flashAttention,
+		RPCServers:     *rpcServers,
 	}
 
 	go server.load(ctx, *mpath, params, lpaths, *parallel, *kvCacheType, *kvSize, *multiUserCache)
diff --git a/server/routes.go b/server/routes.go
index 40348e73..f38e0105 100644
--- a/server/routes.go
+++ b/server/routes.go
@@ -1321,6 +1321,9 @@ func Serve(ln net.Listener) error {
 	// At startup we retrieve GPU information so we can get log messages before loading a model
 	// This will log warnings to the log in case we have problems with detected GPUs
 	gpus := discover.GetGPUInfo()
+	if rpcServers := envconfig.RPCServers(); rpcServers != "" {
+		gpus = append(gpus, discover.GetRPCServers(rpcServers)...)
+	}
 	gpus.LogDetails()
 
 	err = srvr.Serve(ln)
diff --git a/server/sched.go b/server/sched.go
index 2842bb3a..80e18faa 100644
--- a/server/sched.go
+++ b/server/sched.go
@@ -164,6 +164,9 @@ func (s *Scheduler) processPending(ctx context.Context) {
 						gpus = s.getCpuFn()
 					} else {
 						gpus = s.getGpuFn()
+						if rpcServers := envconfig.RPCServers(); rpcServers != "" {
+							gpus = append(gpus, discover.GetRPCServers(rpcServers)...)
+						}
 					}
 
 					if envconfig.MaxRunners() <= 0 {
@@ -227,9 +230,6 @@ func (s *Scheduler) processPending(ctx context.Context) {
 						g := pickBestFullFitByLibrary(pending, ggml, gpus, &numParallel)
 						if g != nil {
 							gpus = g
-						} else {
-							// Only allow partial loads when this is the first model
-							gpus = pickBestPartialFitByLibrary(pending, ggml, gpus, &numParallel)
 						}
 						s.loadFn(pending, ggml, gpus, numParallel)
 						break
@@ -636,9 +636,21 @@ func (runner *runnerRef) needsReload(ctx context.Context, req *LlmRequest) bool
 
 	ctx, cancel := context.WithTimeout(ctx, timeout)
 	defer cancel()
+
+	if !reflect.DeepEqual(runner.Options.RPCServers, req.opts.RPCServers) {
+		slog.Info(
+			"RPC servers changed",
+			"model", runner.model.Name,
+			"new", req.opts.RPCServers,
+			"previous", runner.Options.RPCServers,
+		)
+		return true
+	}
+
 	if !reflect.DeepEqual(runner.model.AdapterPaths, req.model.AdapterPaths) || // have the adapters changed?
 		!reflect.DeepEqual(runner.model.ProjectorPaths, req.model.ProjectorPaths) || // have the projectors changed?
 		!reflect.DeepEqual(optsExisting, optsNew) || // have the runner options changed?
+
 		runner.llama.Ping(ctx) != nil {
 		return true
 	}
-- 
2.39.5 (Apple Git-154)

